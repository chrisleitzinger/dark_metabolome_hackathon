---
title: "Machine learning models with workflowsets"
author: "Christelle Colin-Leitzinger"
date: '`r Sys.Date()`'
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: false
    theme: united
    highlight: pygments
    df_print: paged
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
.figure {
   margin-top: 25px;
   margin-bottom: 10px;
}

table {
    margin-top: 10px;
    margin-bottom: 25px;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      cache = FALSE,
                      fig.align='center'
                      )
```


```{r library}
library(tidyverse)
library(tidymodels)
library(themis)
library(discrim)
library(klaR)
```


```{r load}
# Samples, retention time and m/z data
omics_data <- read_delim(paste0(here::here(), "/lusq_reprocess_2021-12-08_iron_log2_merged.txt")) %>% 
  janitor::clean_names()
#
metabolites_data <- read_delim(paste0(here::here(), "/hmdb_keep_v3_python_blessed.txt")) %>% 
  janitor::clean_names()
```

# I.Prediction using Samples, retention time and m/z information as predictors.

We are trying to predict metabolite classifications. In our proteomic analysis, about 88% of the metabolites are not identified. We want to see if we can predict with a good accuracy the classification of these metabolites based on retention time and m/z information. We also want to access if using patients samples as predictors in addition to theses 2 parameters can increase the prediction.  

## Data preprocessing

- We are focusing on predicting if a metabolite is a lipid or not. I am using a variable I created, `is_lipids`, as the outcome.

- Patient samples present missing values :
The data is imputed with minimum of the expression.  

```{r data prep}
omics_data <- omics_data %>% 
  # Imputation samples data
  mutate(across((starts_with("x06")), .fns = ~ replace_na(., min(., na.rm = TRUE)))) %>% 
  # cleaning retention time to eliminate false detection during set up, washing and equilibrating the HPLC
  filter(row_retention_time > 0.5 & row_retention_time < 14)

mldata <- omics_data %>% 
  # Filter identified metabolites
  filter(non_heavy_identified_flag == 1 & !is.na(hmdb) & !str_detect(hmdb, "\\|")) %>% 
  # Join with known metabolites
  inner_join(., metabolites_data %>% 
              dplyr::select(accession, taxonomy_super_class),
            by = c("hmdb" = "accession")) %>% 
  filter(!is.na(taxonomy_super_class))

unknown_metabolites <- omics_data %>% 
  # Filter identified metabolites
  filter(non_heavy_identified_flag != 1) %>% 
  dplyr::select(hmdb, row_id, 
         row_m_z, row_retention_time,
         starts_with("x06"))
```

```{r}
# skimr::skim(mldata)
mldata <- mldata %>% 
  dplyr::select(hmdb, row_m_z, row_retention_time, 
         starts_with("x06"), taxonomy_super_class) %>% 
  group_by(taxonomy_super_class) %>% 
  mutate(number_sample_class = n()) %>% 
  ungroup() %>% 
  # mutate(taxonomy_super_class = case_when(
  #   number_sample_class <= 5               ~ "Others",
  #   TRUE                                   ~ taxonomy_super_class
  # )) %>% 
  # filter(number_sample_class > 5) %>% 
  # filter(str_detect(taxonomy_super_class, "Lipids|acids")) %>%
  mutate(is_lipids = case_when(
    str_detect(taxonomy_super_class, "Lipids")              ~ "Yes",
    TRUE                                                    ~ "No"
  )) %>% 
  dplyr::select(-number_sample_class, -taxonomy_super_class) %>% 
  mutate_if(is.character, factor)
```


## Build a recipe
We need to take a first look at the data

### Is the data normalized?
```{r}
# skimr::skim(mldata)
summary(mldata)
```

### Is the data balanced?
```{r}
mldata %>% 
  ggplot(aes(x=is_lipids, fill= is_lipids))+
  geom_bar()+
  theme_minimal()
```

```{r}
set.seed(123)

# 1. Splitting the data
# 3/4 of the data into the training set but split evenly winthin race
data_split <- initial_split(mldata, prop = 3/4, strata = is_lipids)
# Create training and testing datasets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

### What are the steps to take?

- Investigate if the data needs to be balanced and/or normalized :
1- Normalize numeric data to have a standard deviation of one or *scale*.
2- Normalize numeric data to have a standard deviation of one and a mean of zero *normalize*.
3- Balanced outcomes with *smote*. This use nearest neighbor to create new synthetic observation almost similar.  

So I initialize 5 machine learning recipes : balanced, scaled, normalized, balanced_scaled, balanced_normalized
```{r recipe}
recipe_smote <- 
  # 1.model formula
  recipe(is_lipids ~ ., data = train_data)  %>% 
   # 2.keep these variables but not use them as either outcomes or predictors
  update_role(hmdb, new_role = "ID") %>% 
  # 3. Set differential steps
  step_smote(is_lipids)

recipe_scale <- 
  recipe(is_lipids ~ ., data = train_data)  %>% 
  update_role(hmdb, new_role = "ID") %>% 
  # 3. Set differential steps
  step_scale(all_numeric_predictors())

recipe_norm <- 
  recipe(is_lipids ~ ., data = train_data)  %>% 
  update_role(hmdb, new_role = "ID") %>% 
  # 3. Set differential steps
  step_normalize(all_numeric_predictors())

recipe_corr <- 
  recipe(is_lipids ~ ., data = train_data)  %>% 
  update_role(hmdb, new_role = "ID") %>% 
  # 3. Set differential steps
  step_corr(all_numeric_predictors())
  
recipe_scale_smote <- 
  recipe(is_lipids ~ ., data = train_data)  %>% 
  update_role(hmdb, new_role = "ID") %>% 
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>% 
  step_smote(is_lipids)

recipe_norm_smote <- 
  recipe(is_lipids ~ ., data = train_data)  %>% 
  update_role(hmdb, new_role = "ID") %>% 
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(is_lipids)

recipe_corr_smote <- 
  recipe(is_lipids ~ ., data = train_data)  %>% 
  update_role(hmdb, new_role = "ID") %>% 
  # 3. Set differential steps
  step_corr(all_numeric_predictors()) %>% 
  step_smote(is_lipids)

recipe_scale_corr <- 
  recipe(is_lipids ~ ., data = train_data)  %>% 
  update_role(hmdb, new_role = "ID") %>% 
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors())

recipe_norm_corr <- 
  recipe(is_lipids ~ ., data = train_data)  %>% 
  update_role(hmdb, new_role = "ID") %>% 
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors())

recipe_norm_smote_corr <- 
  recipe(is_lipids ~ ., data = train_data)  %>% 
  update_role(hmdb, new_role = "ID") %>% 
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors()) %>% 
  step_smote(is_lipids)

recipe_scale_smote_corr <- 
  recipe(is_lipids ~ ., data = train_data)  %>% 
  update_role(hmdb, new_role = "ID") %>% 
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>% 
  step_corr(all_numeric_predictors()) %>% 
  step_smote(is_lipids)
```

```{r}
recipe_list <- 
list(balanced = recipe_smote, 
     scaled = recipe_scale, normalized = recipe_norm,
     corr.removed = recipe_corr,
     balanced.scaled = recipe_scale_smote, balanced.normalized = recipe_norm_smote,
     balanced.corr = recipe_corr_smote,
     scaled.corr = recipe_scale_corr, normalized.corr = recipe_norm_corr,
     balanced.normalized.corr = recipe_norm_smote_corr,
     balanced.scaled.corr = recipe_scale_smote_corr)
#Generate List of Model Types
```


I will be using these 5 recipes on multiple classification models in order or evaluate the recipe / model with the best accuracy :

- bayesian model, decision tree, support-vector machine, ramdom forest, boosted tree, logistic regression, nearest neighbor.

We then will test these workflows on a training dataset.

```{r}
nb_spec <- 
  naive_Bayes(smoothness = tune(), Laplace = tune()) %>% 
  set_engine("klaR") %>% 
  set_mode("classification")
```

```{r}
dt_spec <- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
```

```{r}
svm_spec <- 
  svm_rbf(cost = tune(), rbf_sigma = tune(), margin = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")
```

```{r rand_forest}
set.seed(345)

ranger_spec <- rand_forest(
  mtry = tune(), 
  min_n = tune(),
  trees = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")
```

```{r xgboost}
xgboost_spec <- 
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
             loss_reduction = tune(), sample_size = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("xgboost") 
# xgboost_workflow <- 
#   workflow() %>% 
#   add_recipe(mldata_recipe) %>% 
#   add_model(xgboost_spec) 
# set.seed(345)
# xgboost_tune <-
#   tune_grid(xgboost_workflow, resamples = mldata_folds, grid = 10)
```

```{r}
glmnet_spec <- 
  logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_mode("classification") %>% 
  set_engine("glmnet") 
# glmnet_workflow <- 
#   workflow() %>% 
#   add_recipe(mldata_recipe) %>% 
#   add_model(glmnet_spec) 
# glmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0, 0.05, 
#                                                                                       0.2, 0.4, 0.6, 0.8, 1)) 
# set.seed(345)
# glmnet_tune <- 
#   tune_grid(glmnet_workflow, resamples = mldata_folds, grid = glmnet_grid) 
```

```{r}
knn_spec <- 
  nearest_neighbor(neighbors = tune(), weight_func = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kknn") 
# knn_workflow <- 
#   workflow() %>% 
#   add_recipe(mldata_recipe) %>% 
#   add_model(knn_spec) 
# set.seed(345)
# knn_tune <-
#   tune_grid(knn_workflow, 
#             resamples = mldata_folds, 20)
```

```{r}
model_list <- 
list(Random_Forest = ranger_spec, SVM = svm_spec, Naive_Bayes = nb_spec, 
     Decision_Tree = dt_spec, Boosted_Trees = xgboost_spec, KNN = knn_spec, Logistic_Regression = glmnet_spec)
```

```{r}
model_set <- workflow_set(preproc = recipe_list, models = model_list, cross = T)
```

### Machine learning data sets.

The known metabolites data is split in to a training dataset (75%) and testing dataset (25%). The split is done evenly within the outcome variable `is_lipids`.

- Use a 10 fold cross validation to train and evaluate model on the training set. Resampling procedure will allow us to fit each workflow to the training data and compute accuracy on each resample and take the mean to pick the best model / recipe cross combination.

```{r}
doParallel::registerDoParallel()
set.seed(123)

mldata_folds <- vfold_cv(train_data, strata = is_lipids)
doParallel::registerDoParallel()
all_workflows <- 
  model_set %>% workflow_map(resamples = mldata_folds, 
                             verbose = TRUE)
```

### Visualize performance comparison of workflows
```{r}
collect_metrics(all_workflows) %>% 
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>% 
  filter(.metric == "accuracy") %>% 
  group_by(wflow_id) %>% 
  filter(mean == max(mean)) %>% 
  group_by(model) %>% 
  dplyr::select(-.config) %>% 
  distinct() %>%
  ungroup() %>% 
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  mutate(Model_Type = factor(Model_Type, 
                             levels = c("Random_Forest", "Logistic_Regression", 
                                        "Decision_Tree", "Boosted_Trees",
                                        "KNN", "SVM", "Naive_Bayes"))) %>% 
  ggplot(aes(x=Workflow_Rank, y = mean, shape = Recipe, color = Model_Type)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err)) +
  geom_hline(aes(yintercept = 0.92), color= "darkgrey", linetype= 2)+
  theme_minimal()+
  scale_colour_brewer(palette = "Set1")+
  scale_shape_manual(values=c(0, 1, 2, 3, 15, 16, 17, 18, 7, 25, 8))+
  # scale_colour_viridis_d() +
  labs(title = "Performance Comparison of Workflow Sets", x = "Workflow Rank", y = "Accuracy", color = "Model Types", shape = "Recipes")

collect_metrics(all_workflows) %>% 
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>% 
  filter(.metric == "accuracy") %>% 
  group_by(wflow_id) %>% 
  filter(mean == max(mean)) %>% 
  group_by(model) %>% 
  dplyr::select(-.config) %>% 
  distinct() %>%
  ungroup() %>% 
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  mutate(Model_Type = factor(Model_Type, 
                             levels = c("Random_Forest", "Logistic_Regression", 
                                        "Decision_Tree", "Boosted_Trees",
                                        "KNN", "SVM"))) %>% 
  filter(!is.na(Model_Type)) %>% 
  ggplot(aes(x=Recipe, y = mean, shape = Recipe, color = Model_Type)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err),
                width = 0.1) +
  geom_hline(aes(yintercept = 0.92), color= "darkgrey", linetype= 2)+
  theme_minimal()+
  scale_colour_brewer(palette = "Set1")+
  scale_shape_manual(values=c(0, 1, 2, 3, 15, 16, 17, 18, 7, 25, 8))+
  labs(title = "Performance Comparison of Workflow Sets", x = "Workflow Rank", y = "Accuracy", color = "Model Types", shape = "Recipes")+
  facet_wrap(. ~ Model_Type)
```


# Best model for the choosen recipe 

We saw that best **accuracy** can be obtained with recipes either correlated variables with or without normalizing the data.
I choose to continue with the recipe which change the less the data.

corr.removed    Logistic_Regression  logistic_reg ACCURACY     0.926

corr.removed    Random_Forest         rand_forest  ACCURACY      0.922

```{r chossen recipe}

```

## Build  models

```{r rand_forest 2}
# Model specification
set.seed(345)

ranger_spec <- rand_forest(
  mtry = tune(), 
  min_n = tune(),
  trees = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")

# Set up a workflow container
ranger_workflow <- 
  workflow() %>% 
  # Add preprocessor
  add_recipe(recipe_corr) %>%
  add_model(ranger_spec)

# Tuning
# doParallel::registerDoParallel()

set.seed(345)

ranger_tune <-
  tune_grid(ranger_workflow, 
            resamples = mldata_folds, 
            grid = 20) # Try the 20 candidate point with each min_n and mtry
```

```{r glmnet 2}
glmnet_spec <- 
  logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_mode("classification") %>% 
  set_engine("glmnet") 
glmnet_workflow <- 
  workflow() %>% 
  add_recipe(recipe_corr) %>% 
  add_model(glmnet_spec) 
glmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0, 0.05, 
                                                                                      0.2, 0.4, 0.6, 0.8, 1)) 
set.seed(345)
glmnet_tune <- 
  tune_grid(glmnet_workflow, resamples = mldata_folds, grid = glmnet_grid) 
```

```{r Explore Tuning Results}
set.seed(678)

final_rf <- ranger_workflow %>% 
  finalize_workflow(select_best(ranger_tune, "roc_auc"))
final_glmnet <- glmnet_workflow %>% 
  finalize_workflow(select_best(glmnet_tune, "accuracy"))
```

These are the hyperpareameters value on our random forest and logistic regression
```{r hyperpareameters}
final_rf
final_glmnet
```
<br>

***

```{r fit_resamples}
set.seed(789)
rf_results <- final_rf %>% 
  fit_resamples( 
    resamples = mldata_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
    control = control_resamples(save_pred = TRUE, verbose = TRUE)
  )
set.seed(789)
glmnet_results <- final_glmnet %>% 
  fit_resamples(
    resamples = mldata_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
    control = control_resamples(save_pred = TRUE)
  )
```

## Evaluate models
```{r Performance Metrics}
rf_results %>% # Compare both models
  collect_metrics() %>% 
  dplyr::select(".metric", rf_mean = mean) %>% 
  full_join(., glmnet_results %>% 
              collect_metrics() %>% 
              dplyr::select(".metric", glmet_mean = mean), by = ".metric")

rf_results %>% # Compare both models
  collect_metrics() %>% 
  mutate(model = "rf") %>% 
  bind_rows(glmnet_results %>% 
              collect_metrics() %>% 
              mutate(model = "glmnet")) %>% 
  
  ggplot(aes(x= .metric, y=mean, fill = model))+
  geom_bar(stat = "identity",
           position = position_dodge())+
  geom_errorbar(aes(x = .metric,
                    ymin = mean - std_err,
                    ymax = mean + std_err),
                width = 0.2, alpha = 0.5,
                position=position_dodge(width=0.9))+
  theme_minimal()+
  scale_fill_viridis_d(option = "A", begin= 0.2, end = 0.8)
```
roc-auc and accuracy show a really good performance. The sensitivity (ability to detect a true positive) and specificity are good.

```{r evaluate models}
rf_results %>% # Compare both models
  collect_predictions() %>% 
  mutate(model = "rf") %>% 
  bind_rows(glmnet_results %>% 
              collect_predictions() %>% 
              mutate(model = "glmnet")) %>% 
  group_by(model) %>% 
  roc_curve(is_lipids, .pred_No) %>% 
  autoplot()
```
<br>
<br>

```{r evaluate models2}
df <- rf_results %>% 
  collect_predictions()
conf_tab <- table(df$is_lipids, df$.pred_class)
conf_tab <- conf_tab / rowSums(conf_tab)
conf_tab <- as.data.frame(conf_tab, stringsAsFactors = TRUE)
conf_tab$Var2 <- factor(conf_tab$Var2, rev(levels(conf_tab$Var2)))

ggplot(conf_tab, aes(Var1, Var2, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = scales::percent(Freq))) +
  scale_fill_gradient(low = "white", high = "#3575b5")+
  labs(x = "Truth", y = "Prediction", title = "Confusion matrix for Ramdom Forest",
       fill = "Valid prediction")+
  theme_minimal()

rf_results %>%
  collect_predictions() %>%
  conf_mat(is_lipids, .pred_class)

df <- glmnet_results %>% 
  collect_predictions()
conf_tab <- table(df$is_lipids, df$.pred_class)
conf_tab <- conf_tab / rowSums(conf_tab)
conf_tab <- as.data.frame(conf_tab, stringsAsFactors = TRUE)
conf_tab$Var2 <- factor(conf_tab$Var2, rev(levels(conf_tab$Var2)))

ggplot(conf_tab, aes(Var1, Var2, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = scales::percent(Freq))) +
  scale_fill_gradient(low = "white", high = "#3575b5")+
  labs(x = "Truth", y = "Prediction", title = "Confusion matrix for Logistic Regression",
       fill = "Valid prediction")+
  theme_minimal()

glmnet_results %>%
  collect_predictions() %>%
  conf_mat(is_lipids, .pred_class)
```
Overall prediction estimate for ramdom forest on the cross validation set is `r rf_results %>% collect_predictions() %>% ppv(truth = is_lipids, estimate = .pred_class) %>% dplyr::select(.estimate)`  
Overall prediction estimate for logistic regression on the cross validation set is `r glmnet_results %>% collect_predictions() %>% ppv(truth = is_lipids, estimate = .pred_class) %>% dplyr::select(.estimate)`

<br>

***

## Features importance
```{r vip}
############################################################################################### Step Explore of features importance ----
library(vip) 
###################################### glmnet----
# Need to train the model one more time but without tuning to go faster
importance_spec <- glmnet_spec %>% 
  finalize_model(select_best(glmnet_tune, "roc_auc")) %>% 
  set_engine("glmnet", importance = "permutation") # permutation based importance

# represents the mean decrease in node impurity (and not the mean decrease in accuracy)
workflow() %>% 
  add_recipe(recipe_corr) %>% 
  add_model(importance_spec) %>% 
  fit(train_data) %>% 
  extract_fit_parsnip() %>% 
  vip(aesthetics = list(alpha = 0.5, fill = "midnightblue"),
      # geom = "point",
      num_features = 20
  )+
ggtitle("logistic regression")


###################################### Ranger----

# Need to train the model one more time but without tuning to go faster
importance_spec <- ranger_spec %>% 
  finalize_model(select_best(ranger_tune, "roc_auc")) %>% 
  set_engine("ranger", importance = "permutation") # permutation based importance

# represents the mean decrease in node impurity (and not the mean decrease in accuracy)
workflow() %>% 
  add_recipe(recipe_corr) %>% 
  add_model(importance_spec) %>% 
  fit(train_data) %>% 
  extract_fit_parsnip() %>% 
  vip(aesthetics = list(alpha = 0.5, fill = "midnightblue"),
      # geom = "point",
      num_features = 20
  )+
ggtitle("Random forest")
```
<br>

***

## Prediction on test data identified metabolites : Lipids Yes / No (Ramdom forest)
```{r}
############################################################################################### AT LAST
# Step finalize Fit : fitting to the whole training and evaluating on the testing data
# With the model of our choice
set.seed(101)

last_fit <- final_rf %>% 
  last_fit(data_split)


###################

final_fit <- last_fit


collect_metrics(final_fit)
# Compare to the training previous number
collect_metrics(rf_results) # as a meminder of previous results
# Test data is a littke lower with samll SD

collect_metrics(final_fit) %>% 
  mutate(set = "testing") %>% 
  rename(mean = .estimate) %>% 
  bind_rows(collect_metrics(rf_results) %>% 
              mutate(set = "training")) %>% 
  filter(str_detect(.metric, "accuracy|roc_auc")) %>% 
  ggplot(aes(x= .metric, y=mean, fill = set))+
  geom_bar(stat = "identity",
           position = position_dodge()) #+ 
  # geom_errorbar(aes(xmin = mean - std_err,
  #                   xmax = mean + std_err),
  #               width = 0.2, alpha = 0.5)
# collect_metrics(final_fit) %>% 
#   mutate(set = "testing") %>% 
#   rename(mean = .estimate) %>% 
#   bind_rows(collect_metrics(rf_results) %>% 
#               mutate(set = "training")) %>% 
#   filter(str_detect(.metric, "accuracy|roc_auc")) %>% 
#   ggplot(aes(x= .metric, y=mean, color = set))+
#   geom_point() + 
#   geom_errorbar(aes(xmin = mean - std_err,
#                     xmax = mean + std_err),
#                 width = 0.2, alpha = 0.5)


final_fit %>% 
  collect_predictions() %>% 
  conf_mat(truth = is_lipids, estimate = .pred_class)
final_fit %>% 
  collect_predictions() %>% 
  conf_mat(truth = is_lipids, estimate = .pred_class) %>% 
  autoplot()
##############################

# test_fit <- last_fit %>% 
#   predict(test_data)

predicted <- final_fit %>% 
  collect_predictions()

test_data %>% 
  rename(Original_is_lipids = is_lipids) %>% 
  bind_cols(., predicted) %>% 
  rename(predicted_is_lipids = .pred_class) %>% 
  mutate(concordant = case_when(
    Original_is_lipids == predicted_is_lipids      ~ "truly classified",
    Original_is_lipids != predicted_is_lipids      ~ "wrongly classified",
  )) %>% 
  ggplot(aes(x=row_m_z, row_retention_time, color= concordant))+
  geom_point()+
  theme_minimal()+
  facet_wrap(. ~ Original_is_lipids, ncol = 2)
```
<br>

***

## Prediction on non identified metabolites : Lipids Yes / No (Ramdom forest)
```{r fit on testing RF}
set.seed(101)

last_fit <- final_rf %>% 
  fit(train_data)

test_fit <- last_fit %>% 
  predict(unknown_metabolites)

predicted_metabolites <- unknown_metabolites %>% 
  bind_cols(., test_fit) %>% 
  rename(is_lipids = .pred_class) %>% 
  mutate(pred = "prediction") %>% 
  bind_rows(., train_data %>%  
              mutate(pred = "measured"))


predicted_metabolites %>% 
  ggplot(aes(x=row_m_z, row_retention_time, color= pred))+
  geom_point()+
  theme_minimal()+
  facet_wrap(. ~ is_lipids, ncol = 2)
```

## Prediction on non identified metabolites : Lipids Yes / No (Logistic Regression)
The blue points are the unknown metabolites that we can predict to be lipids or not. We can see a pretty good ovelap with the orange point from the known metabolites dataset.
```{r}
############################################################################################### AT LAST
# Step finalize Fit : fitting to the whole training and evaluating on the testing data
# With the model of our choice
set.seed(101)

last_fit <- final_glmnet %>% 
  fit(train_data)

test_fit <- last_fit %>% 
  predict(unknown_metabolites)

predicted_metabolites <- unknown_metabolites %>% 
  bind_cols(., test_fit) %>% 
  rename(is_lipids = .pred_class) %>% 
  mutate(pred = "prediction") %>% 
  bind_rows(., mldata %>%  
              mutate(pred = "measured"))


predicted_metabolites %>% 
  ggplot(aes(x=row_m_z, row_retention_time, color= pred))+
  geom_point()+
  theme_minimal()+
  facet_wrap(. ~ is_lipids, ncol = 2)
```
<br>


# II.Precdition using m/z and retention time only
Again,  

We are trying to predict metabolite classifications. In our proteomic analysis, about 88% of the metabolites are not identified. We want to see if we can predict with a good accuracy the classification of these metabolites based on retention time and m/z information.

## Data preprocessing

- We are focusing on predicting if a metabolite is a lipid or not. I am using a variable I created, `is_lipids`, as the outcome.

```{r data prep without samples}
mldata <- omics_data %>% 
  # Filter identified metabolites
  filter(non_heavy_identified_flag == 1 & !is.na(hmdb) & !str_detect(hmdb, "\\|")) %>% 
  # Join with known metabolites
  inner_join(., metabolites_data %>% 
              dplyr::select(accession, taxonomy_super_class),
            by = c("hmdb" = "accession")) %>% 
  filter(!is.na(taxonomy_super_class))

unknown_metabolites <- omics_data %>% 
  # Filter identified metabolites
  filter(non_heavy_identified_flag != 1) %>% 
  dplyr::select(hmdb, row_id, 
         row_m_z, row_retention_time)
```

```{r}
# skimr::skim(mldata)
mldata <- mldata %>% 
  dplyr::select(hmdb, row_m_z, row_retention_time, 
         taxonomy_super_class) %>% 
  group_by(taxonomy_super_class) %>% 
  mutate(number_sample_class = n()) %>% 
  ungroup() %>% 
  # mutate(taxonomy_super_class = case_when(
  #   number_sample_class <= 5               ~ "Others",
  #   TRUE                                   ~ taxonomy_super_class
  # )) %>% 
  # filter(number_sample_class > 5) %>% 
  # filter(str_detect(taxonomy_super_class, "Lipids|acids")) %>%
  mutate(is_lipids = case_when(
    str_detect(taxonomy_super_class, "Lipids")              ~ "Yes",
    TRUE                                                    ~ "No"
  )) %>% 
  dplyr::select(-number_sample_class, -taxonomy_super_class) %>% 
  mutate_if(is.character, factor)
```


## Build a recipe
We need to take a first look at the data

### Is the data normalized?
```{r}
# skimr::skim(mldata)
summary(mldata)
```

### Is the data balanced?
```{r}
mldata %>% 
  ggplot(aes(x=is_lipids, fill= is_lipids))+
  geom_bar()+
  theme_minimal()
```

```{r}
set.seed(123)

# 1. Splitting the data
# 3/4 of the data into the training set but split evenly winthin race
data_split <- initial_split(mldata, prop = 3/4, strata = is_lipids)
# Create training and testing datasets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

### What are the steps to take?

- Investigate if the data needs to be balanced and/or normalized :
1- Normalize numeric data to have a standard deviation of one or *scale*.
2- Normalize numeric data to have a standard deviation of one and a mean of zero *normalize*.
3- Balanced outcomes with *smote*. This use nearest neighbor to create new synthetic observation almost similar.  

So I initialize the same 5 machine learning recipes : balanced, scaled, normalized, balanced_scaled, balanced_normalized
```{r}
recipe_smote <-
  # 1.model formula
  recipe(is_lipids ~ ., data = train_data)  %>%
   # 2.keep these variables but not use them as either outcomes or predictors
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_smote(is_lipids)

recipe_scale <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors())

recipe_norm <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors())

recipe_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_corr(all_numeric_predictors())

recipe_scale_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_norm_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_corr_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_scale_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors())

recipe_norm_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors())

recipe_norm_smote_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_scale_smote_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)
```

```{r}
recipe_list <- 
list(balanced = recipe_smote, 
     scaled = recipe_scale, normalized = recipe_norm,
     corr.removed = recipe_corr,
     balanced.scaled = recipe_scale_smote, balanced.normalized = recipe_norm_smote,
     balanced.corr = recipe_corr_smote,
     scaled.corr = recipe_scale_corr, normalized.corr = recipe_norm_corr,
     balanced.normalized.corr = recipe_norm_smote_corr,
     balanced.scaled.corr = recipe_scale_smote_corr)
#Generate List of Model Types
```


I will be using these 5 recipes on multiple classification models in order or evaluate the recipe / model with the best accuracy :

- bayesian model, decision tree, support-vector machine, ramdom forest, boosted tree, logistic regression, nearest neighbor.

We then will test these workflows on a training dataset.

<!-- ```{r} -->
<!-- nb_spec <-  -->
<!--   naive_Bayes(smoothness = tune(), Laplace = tune()) %>%  -->
<!--   set_engine("klaR") %>%  -->
<!--   set_mode("classification") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- dt_spec <- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>%  -->
<!--   set_engine("rpart") %>%  -->
<!--   set_mode("classification") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- svm_spec <-  -->
<!--   svm_rbf(cost = tune(), rbf_sigma = tune(), margin = tune()) %>%  -->
<!--   set_engine("kernlab") %>%  -->
<!--   set_mode("classification") -->
<!-- ``` -->

<!-- ```{r rand_forest} -->
<!-- set.seed(345) -->

<!-- ranger_spec <- rand_forest( -->
<!--   mtry = tune(),  -->
<!--   min_n = tune(), -->
<!--   trees = tune()) %>%  -->
<!--   set_mode("classification") %>%  -->
<!--   set_engine("ranger") -->
<!-- ``` -->

<!-- ```{r xgboost} -->
<!-- xgboost_spec <-  -->
<!--   boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(),  -->
<!--              loss_reduction = tune(), sample_size = tune()) %>%  -->
<!--   set_mode("classification") %>%  -->
<!--   set_engine("xgboost")  -->
<!-- ``` -->

<!-- ```{r} -->
<!-- glmnet_spec <-  -->
<!--   logistic_reg(penalty = tune(), mixture = tune()) %>% -->
<!--   set_mode("classification") %>%  -->
<!--   set_engine("glmnet")  -->
<!-- ``` -->

<!-- ```{r} -->
<!-- knn_spec <-  -->
<!--   nearest_neighbor(neighbors = tune(), weight_func = tune()) %>%  -->
<!--   set_mode("classification") %>%  -->
<!--   set_engine("kknn")  -->
<!-- ``` -->

<!-- ```{r} -->
<!-- model_list <-  -->
<!-- list(Random_Forest = ranger_spec, SVM = svm_spec, Naive_Bayes = nb_spec,  -->
<!--      Decision_Tree = dt_spec, Boosted_Trees = xgboost_spec, KNN = knn_spec, Logistic_Regression = glmnet_spec) -->
<!-- ``` -->

```{r}
model_set <- workflow_set(preproc = recipe_list, models = model_list, cross = T)
```

### Machine learning data sets.

The known metabolites data is split in to a training dataset (75%) and testing dataset (25%). The split is done evenly within the outcome variable `is_lipids`.

- Use a 10 fold cross validation to train and evaluate model on the training set. Resampling procedure will allow us to fit each workflow to the training data and compute accuracy on each resample and take the mean to pick the best model / recipe cross combination.

```{r workflow set without samples}
doParallel::registerDoParallel()
set.seed(123)

mldata_folds <- vfold_cv(train_data, strata = is_lipids)
doParallel::registerDoParallel()
all_workflows <- 
  model_set %>% workflow_map(resamples = mldata_folds, 
                             verbose = TRUE)
```

### Visualize performance comparison of workflows
```{r}
collect_metrics(all_workflows) %>% 
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>% 
  filter(.metric == "accuracy") %>% 
  group_by(wflow_id) %>% 
  filter(mean == max(mean)) %>% 
  group_by(model) %>% 
  dplyr::select(-.config) %>% 
  distinct() %>%
  ungroup() %>% 
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  mutate(Model_Type = factor(Model_Type, 
                             levels = c("Random_Forest", "Logistic_Regression", 
                                        "Decision_Tree", "Boosted_Trees",
                                        "KNN", "SVM", "Naive_Bayes"))) %>% 
  ggplot(aes(x=Workflow_Rank, y = mean, shape = Recipe, color = Model_Type)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err)) +
  geom_hline(aes(yintercept = 0.92), color= "darkgrey", linetype= 2)+
  theme_minimal()+
  scale_colour_brewer(palette = "Set1")+
  scale_shape_manual(values=c(0, 1, 2, 3, 15, 16, 17, 18, 7, 25, 8))+
  # scale_colour_viridis_d() +
  labs(title = "Performance Comparison of Workflow Sets", x = "Workflow Rank", y = "Accuracy", color = "Model Types", shape = "Recipes")

collect_metrics(all_workflows) %>% 
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>% 
  filter(.metric == "accuracy") %>% 
  group_by(wflow_id) %>% 
  filter(mean == max(mean)) %>% 
  group_by(model) %>% 
  dplyr::select(-.config) %>% 
  distinct() %>%
  ungroup() %>% 
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  mutate(Model_Type = factor(Model_Type, 
                             levels = c("Random_Forest", "Logistic_Regression", 
                                        "Decision_Tree", "Boosted_Trees",
                                        "KNN", "SVM"))) %>% 
  filter(!is.na(Model_Type)) %>% 
  ggplot(aes(x=Recipe, y = mean, shape = Recipe, color = Model_Type)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err),
                width = 0.1) +
  geom_hline(aes(yintercept = 0.92), color= "darkgrey", linetype= 2)+
  theme_minimal()+
  scale_colour_brewer(palette = "Set1")+
  scale_shape_manual(values=c(0, 1, 2, 3, 15, 16, 17, 18, 7, 25, 8))+
  labs(title = "Performance Comparison of Workflow Sets", x = "Workflow Rank", y = "Accuracy", color = "Model Types", shape = "Recipes")+
  facet_wrap(. ~ Model_Type)
```


<!-- ## Data -->
<!-- The data is imputed with minimum of the expression.   -->

<!-- We are using the known metabolites in samples to built the machine learning dataset. -->
<!-- ```{r data prep 2} -->
<!-- mldata <- omics_data %>%  -->
<!--   # Filter identified metabolites -->
<!--   filter(non_heavy_identified_flag == 1 & !is.na(hmdb) & !str_detect(hmdb, "\\|")) %>%  -->
<!--   # Join with known metabolites -->
<!--   inner_join(., metabolites_data %>%  -->
<!--               dplyr::select(accession, taxonomy_super_class), -->
<!--             by = c("hmdb" = "accession")) %>%  -->
<!--   filter(!is.na(taxonomy_super_class)) -->

<!-- unknown_metabolites <- omics_data %>%  -->
<!--   # Filter identified metabolites -->
<!--   filter(non_heavy_identified_flag != 1) %>%  -->
<!--   dplyr::select(hmdb, row_id,  -->
<!--          row_m_z, row_retention_time) -->
<!-- ``` -->

<!-- I created a variable `is_lipids` to use as the outcome. We will use retention time and m/z information as predictors. -->
<!-- ```{r} -->
<!-- # skimr::skim(mldata) -->
<!-- mldata <- mldata %>%  -->
<!--   dplyr::select(hmdb, row_m_z, row_retention_time,  -->
<!--          taxonomy_super_class) %>%  -->
<!--   group_by(taxonomy_super_class) %>%  -->
<!--   mutate(number_sample_class = n()) %>%  -->
<!--   ungroup() %>%  -->
<!--   # mutate(taxonomy_super_class = case_when( -->
<!--   #   number_sample_class <= 5               ~ "Others", -->
<!--   #   TRUE                                   ~ taxonomy_super_class -->
<!--   # )) %>%  -->
<!--   # filter(number_sample_class > 5) %>%  -->
<!--   # filter(str_detect(taxonomy_super_class, "Lipids|acids")) %>% -->
<!--   mutate(is_lipids = case_when( -->
<!--     str_detect(taxonomy_super_class, "Lipids")              ~ "Yes", -->
<!--     TRUE                                                    ~ "No" -->
<!--   )) %>%  -->
<!--   dplyr::select(-number_sample_class, -taxonomy_super_class) %>%  -->
<!--   mutate_if(is.character, factor) -->

<!-- # skimr::skim(train_data) -->
<!-- # summary(train_data) -->
<!-- ``` -->

<!-- The data is split in to a training dataset (75%) and testing dataset (25%). -->

<!-- ```{r} -->
<!-- set.seed(123) -->

<!-- # 1. Splitting the data -->
<!-- # 3/4 of the data into the training set but split evenly winthin race -->
<!-- data_split <- initial_split(mldata, prop = 3/4, strata = is_lipids) -->
<!-- # Create training and testing datasets: -->
<!-- train_data <- training(data_split) -->
<!-- test_data  <- testing(data_split) -->
<!-- ``` -->

<!-- ## Build a model -->
<!-- I use 10 fold cross validation datasets from the training to evaluate the performance of the models.   -->


<!-- ### Build a recipe -->

<!-- ```{r recipe 2} -->
<!-- # Recipe : data pre processing and features engineering + imputation -->
<!-- mldata_recipe <- -->
<!--   # 1.model formula -->
<!--   recipe(is_lipids ~ ., data = train_data)  %>%  -->
<!--   # 2.keep these variables but not use them as either outcomes or predictors -->
<!--   update_role(hmdb, new_role = "ID") %>%  -->
<!--   # Use nearest neighbor to create new synthetic observation almost similar -->
<!--   step_smote(is_lipids) -->
<!-- mldata_recipe -->
<!-- ``` -->
<!-- - 1 Id = hmdb, 1 outcome = is_lipids, 89 predictors = row_m_z + row_retention_time + samples   -->
<!-- - the models use `step_smote` to balance the outcome cases in the training set. This use nearest neighbor to create new synthetic observation almost similar.   -->
<!-- - Use a 10 fold cross validation to train and evaluate model on the training set This allow us to pick the best values for the hyperparameters of the random forest model   -->
<!-- ```{r} -->
<!-- set.seed(123) -->

<!-- mldata_folds <- vfold_cv(train_data, strata = is_lipids) -->
<!-- ``` -->

<!-- <!-- ## Build model --> -->

<!-- <!-- # RANDOM FOREST --> -->
<!-- ```{r rand_forest 2} -->
<!-- # Model specification -->
<!-- set.seed(345) -->

<!-- ranger_spec <- rand_forest( -->
<!--   mtry = tune(),  -->
<!--   min_n = tune(), -->
<!--   trees = tune()) %>%  -->
<!--   set_mode("classification") %>%  -->
<!--   set_engine("ranger") -->

<!-- # Set up a workflow container -->
<!-- ranger_workflow <-  -->
<!--   workflow() %>%  -->
<!--   # Add preprocessor -->
<!--   add_recipe(mldata_recipe) %>% -->
<!--   add_model(ranger_spec) -->

<!-- # Tuning -->
<!-- # doParallel::registerDoParallel() -->

<!-- set.seed(345) -->

<!-- ranger_tune <- -->
<!--   tune_grid(ranger_workflow,  -->
<!--             resamples = mldata_folds,  -->
<!--             grid = 20) # Try the 20 candidate point with each min_n and mtry -->
<!-- ``` -->


<!-- ```{r xgboost 2} -->
<!-- xgboost_spec <-  -->
<!--   boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(),  -->
<!--              loss_reduction = tune(), sample_size = tune()) %>%  -->
<!--   set_mode("classification") %>%  -->
<!--   set_engine("xgboost")  -->
<!-- xgboost_workflow <-  -->
<!--   workflow() %>%  -->
<!--   add_recipe(mldata_recipe) %>%  -->
<!--   add_model(xgboost_spec)  -->
<!-- set.seed(345) -->
<!-- xgboost_tune <- -->
<!--   tune_grid(xgboost_workflow, resamples = mldata_folds, grid = 10) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- glmnet_spec <-  -->
<!--   logistic_reg(penalty = tune(), mixture = tune()) %>% -->
<!--   set_mode("classification") %>%  -->
<!--   set_engine("glmnet")  -->
<!-- glmnet_workflow <-  -->
<!--   workflow() %>%  -->
<!--   add_recipe(mldata_recipe) %>%  -->
<!--   add_model(glmnet_spec)  -->
<!-- glmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0, 0.05,  -->
<!--                                                                                       0.2, 0.4, 0.6, 0.8, 1))  -->
<!-- set.seed(345) -->
<!-- glmnet_tune <-  -->
<!--   tune_grid(glmnet_workflow, resamples = mldata_folds, grid = glmnet_grid)  -->
<!-- ``` -->

<!-- ```{r} -->
<!-- knn_spec <-  -->
<!--   nearest_neighbor(neighbors = tune(), weight_func = tune()) %>%  -->
<!--   set_mode("classification") %>%  -->
<!--   set_engine("knn")  -->
<!-- knn_workflow <-  -->
<!--   workflow() %>%  -->
<!--   add_recipe(mldata_recipe) %>%  -->
<!--   add_model(knn_spec)  -->
<!-- set.seed(345) -->
<!-- knn_tune <- -->
<!--   tune_grid(knn_workflow,  -->
<!--             resamples = mldata_folds, 20) -->
<!-- ``` -->


<!-- ```{r Explore Tuning Results 2} -->
<!-- set.seed(678) -->

<!-- final_rf <- ranger_workflow %>%  -->
<!--   finalize_workflow(dplyr::select_best(ranger_tune, "accuracy")) -->
<!-- final_xgboost <- xgboost_workflow %>%  -->
<!--   finalize_workflow(dplyr::select_best(xgboost_tune, "roc_auc")) -->
<!-- final_glmnet <- glmnet_workflow %>%  -->
<!--   finalize_workflow(dplyr::select_best(glmnet_tune, "roc_auc")) -->
<!-- final_knn <- knn_workflow %>%  -->
<!--   finalize_workflow(dplyr::select_best(knn_tune, "roc_auc")) -->
<!-- ``` -->

<!-- These are the hyperpareameters value on our random forest and logistic regression -->
<!-- ```{r hyperpareameters 2} -->
<!-- final_rf -->
<!-- final_glmnet -->
<!-- ``` -->
<!-- <br> -->

<!-- *** -->

<!-- ```{r fit_resamples 2} -->
<!-- set.seed(789) -->
<!-- rf_results <- final_rf %>%  -->
<!--   fit_resamples(  -->
<!--     resamples = mldata_folds, -->
<!--     metrics = metric_set(roc_auc, accuracy, sensitivity, specificity), -->
<!--     control = control_resamples(save_pred = TRUE, verbose = TRUE) -->
<!--   ) -->
<!-- set.seed(789) -->
<!-- xgboost_results <- final_xgboost %>%  -->
<!--   fit_resamples( -->
<!--     resamples = mldata_folds, -->
<!--     metrics = metric_set(roc_auc, accuracy, sensitivity, specificity), -->
<!--     control = control_resamples(save_pred = TRUE) -->
<!--   ) -->
<!-- set.seed(789) -->
<!-- glmnet_results <- final_glmnet %>%  -->
<!--   fit_resamples( -->
<!--     resamples = mldata_folds, -->
<!--     metrics = metric_set(roc_auc, accuracy, sensitivity, specificity), -->
<!--     control = control_resamples(save_pred = TRUE) -->
<!--   ) -->
<!-- set.seed(789) -->
<!-- knn_results <- final_knn %>%  -->
<!--   fit_resamples( -->
<!--     resamples = mldata_folds, -->
<!--     metrics = metric_set(roc_auc, accuracy, sensitivity, specificity), -->
<!--     control = control_resamples(save_pred = TRUE) -->
<!--   ) -->
<!-- ``` -->

<!-- ## Evaluate models -->
<!-- ```{r Performance Metrics 2} -->
<!-- rf_results %>% # Compare both models -->
<!--   collect_metrics() %>%  -->
<!--   dplyr::select(".metric", rf_mean = mean) %>%  -->
<!--   full_join(., glmnet_results %>%  -->
<!--               collect_metrics() %>%  -->
<!--               dplyr::select(".metric", glmet_mean = mean), by = ".metric") %>%  -->
<!--   full_join(., xgboost_results %>%  -->
<!--               collect_metrics() %>%  -->
<!--               dplyr::select(".metric", xgboost_mean = mean), by = ".metric") %>%  -->
<!--   full_join(., knn_results %>%  -->
<!--               collect_metrics() %>%  -->
<!--               dplyr::select(".metric", knn_mean = mean), by = ".metric") -->

<!-- rf_results %>% # Compare both models -->
<!--   collect_metrics() %>%  -->
<!--   mutate(model = "rf") %>%  -->
<!--   bind_rows(glmnet_results %>%  -->
<!--               collect_metrics() %>%  -->
<!--               mutate(model = "glmnet")) %>%  -->
<!--   bind_rows(xgboost_results %>%  -->
<!--               collect_metrics() %>%  -->
<!--               mutate(model = "xgboost")) %>%  -->
<!--   bind_rows(knn_results %>%  -->
<!--               collect_metrics() %>%  -->
<!--               mutate(model = "knn")) %>%  -->

<!--   ggplot(aes(x= .metric, y=mean, fill = model))+ -->
<!--   geom_bar(stat = "identity", -->
<!--            position = position_dodge())+ -->
<!--   geom_errorbar(aes(x = .metric, -->
<!--                     ymin = mean - std_err, -->
<!--                     ymax = mean + std_err), -->
<!--                 width = 0.2, alpha = 0.5, -->
<!--                 position=position_dodge(width=0.9))+ -->
<!--   theme_minimal()+ -->
<!--   scale_fill_viridis_d(option = "A") -->
<!-- ``` -->
<!-- roc-auc and accuracy show a really good performance. The sensitivity (ability to detect a true positive) and specificity are good. -->

<!-- ```{r evaluate models 2} -->
<!-- rf_results %>% # Compare both models -->
<!--   collect_predictions() %>%  -->
<!--   mutate(model = "rf") %>%  -->
<!--   bind_rows(glmnet_results %>%  -->
<!--               collect_predictions() %>%  -->
<!--               mutate(model = "glmnet")) %>%  -->
<!--   bind_rows(xgboost_results %>%  -->
<!--               collect_predictions() %>%  -->
<!--               mutate(model = "xgboost")) %>%  -->
<!--   bind_rows(knn_results %>%  -->
<!--               collect_predictions() %>%  -->
<!--               mutate(model = "knn")) %>%  -->
<!--   group_by(model) %>%  -->
<!--   roc_curve(is_lipids, .pred_No) %>%  -->
<!--   autoplot() -->
<!-- ``` -->
<!-- <br> -->
<!-- <br> -->

<!-- ```{r evaluate models2-2} -->
<!-- df <- rf_results %>%  -->
<!--   collect_predictions() -->
<!-- conf_tab <- table(df$is_lipids, df$.pred_class) -->
<!-- conf_tab <- conf_tab / rowSums(conf_tab) -->
<!-- conf_tab <- as.data.frame(conf_tab, stringsAsFactors = TRUE) -->
<!-- conf_tab$Var2 <- factor(conf_tab$Var2, rev(levels(conf_tab$Var2))) -->

<!-- ggplot(conf_tab, aes(Var1, Var2, fill = Freq)) + -->
<!--   geom_tile() + -->
<!--   geom_text(aes(label = scales::percent(Freq))) + -->
<!--   scale_fill_gradient(low = "white", high = "#3575b5")+ -->
<!--   labs(x = "Truth", y = "Prediction", title = "Confusion matrix for Ramdom Forest", -->
<!--        fill = "Valid prediction")+ -->
<!--   theme_minimal() -->

<!-- rf_results %>% -->
<!--   collect_predictions() %>% -->
<!--   conf_mat(is_lipids, .pred_class) -->

<!-- df <- glmnet_results %>%  -->
<!--   collect_predictions() -->
<!-- conf_tab <- table(df$is_lipids, df$.pred_class) -->
<!-- conf_tab <- conf_tab / rowSums(conf_tab) -->
<!-- conf_tab <- as.data.frame(conf_tab, stringsAsFactors = TRUE) -->
<!-- conf_tab$Var2 <- factor(conf_tab$Var2, rev(levels(conf_tab$Var2))) -->

<!-- ggplot(conf_tab, aes(Var1, Var2, fill = Freq)) + -->
<!--   geom_tile() + -->
<!--   geom_text(aes(label = scales::percent(Freq))) + -->
<!--   scale_fill_gradient(low = "white", high = "#3575b5")+ -->
<!--   labs(x = "Truth", y = "Prediction", title = "Confusion matrix for Logistic Regression", -->
<!--        fill = "Valid prediction")+ -->
<!--   theme_minimal() -->

<!-- glmnet_results %>% -->
<!--   collect_predictions() %>% -->
<!--   conf_mat(is_lipids, .pred_class) -->
<!-- ``` -->
<!-- Overall prediction estimate for ramdom forest on the cross validation set is `r rf_results %>% collect_predictions() %>% ppv(truth = is_lipids, estimate = .pred_class) %>% dplyr::select(.estimate)`   -->
<!-- Overall prediction estimate for logistic regression on the cross validation set is `r glmnet_results %>% collect_predictions() %>% ppv(truth = is_lipids, estimate = .pred_class) %>% dplyr::select(.estimate)` -->

<!-- <br> -->

<!-- *** -->

<!-- ## Features importance -->
<!-- ```{r vip 2} -->
<!-- ############################################################################################### Step Explore of features importance ---- -->
<!-- library(vip)  -->
<!-- ###################################### glmnet---- -->
<!-- # Need to train the model one more time but without tuning to go faster -->
<!-- importance_spec <- glmnet_spec %>%  -->
<!--   finalize_model(dplyr::select_best(glmnet_tune, "roc_auc")) %>%  -->
<!--   set_engine("glmnet", importance = "permutation") # permutation based importance -->

<!-- # represents the mean decrease in node impurity (and not the mean decrease in accuracy) -->
<!-- workflow() %>%  -->
<!--   add_recipe(mldata_recipe) %>%  -->
<!--   add_model(importance_spec) %>%  -->
<!--   fit(train_data) %>%  -->
<!--   extract_fit_parsnip() %>%  -->
<!--   vip(aesthetics = list(alpha = 0.5, fill = "midnightblue"), -->
<!--       # geom = "point", -->
<!--       num_features = 20 -->
<!--   )+ -->
<!-- ggtitle("logistic regression") -->


<!-- ###################################### Ranger---- -->

<!-- # Need to train the model one more time but without tuning to go faster -->
<!-- importance_spec <- ranger_spec %>%  -->
<!--   finalize_model(dplyr::select_best(ranger_tune, "roc_auc")) %>%  -->
<!--   set_engine("ranger", importance = "permutation") # permutation based importance -->

<!-- # represents the mean decrease in node impurity (and not the mean decrease in accuracy) -->
<!-- workflow() %>%  -->
<!--   add_recipe(mldata_recipe) %>%  -->
<!--   add_model(importance_spec) %>%  -->
<!--   fit(train_data) %>%  -->
<!--   extract_fit_parsnip() %>%  -->
<!--   vip(aesthetics = list(alpha = 0.5, fill = "midnightblue"), -->
<!--       # geom = "point", -->
<!--       num_features = 20 -->
<!--   )+ -->
<!-- ggtitle("Random forest") -->
<!-- ``` -->
<!-- <br> -->

<!-- *** -->

<!-- <br> -->

<!-- ## Prediction on test data identified metabolites : Lipids Yes / No (Ramdom forest) -->
<!-- ```{r} -->
<!-- ############################################################################################### AT LAST -->
<!-- # Step finalize Fit : fitting to the whole training and evaluating on the testing data -->
<!-- # With the model of our choice -->
<!-- set.seed(101) -->

<!-- last_fit <- final_rf %>%  -->
<!--   last_fit(data_split) -->


<!-- ################### -->

<!-- final_fit <- last_fit -->


<!-- collect_metrics(final_fit) -->
<!-- # Compare to the training previous number -->
<!-- collect_metrics(rf_results) # as a meminder of previous results -->
<!-- # Test data is a littke lower with samll SD -->

<!-- collect_metrics(final_fit) %>%  -->
<!--   mutate(set = "testing") %>%  -->
<!--   rename(mean = .estimate) %>%  -->
<!--   bind_rows(collect_metrics(rf_results) %>%  -->
<!--               mutate(set = "training")) %>%  -->
<!--   filter(str_detect(.metric, "accuracy|roc_auc")) %>%  -->
<!--   ggplot(aes(x= .metric, y=mean, fill = set))+ -->
<!--   geom_bar(stat = "identity", -->
<!--            position = position_dodge()) +  -->
<!--   geom_errorbar(aes(xmin = mean - std_err, -->
<!--                     xmax = mean + std_err), -->
<!--                 width = 0.2, alpha = 0.5) -->
<!-- collect_metrics(final_fit) %>%  -->
<!--   mutate(set = "testing") %>%  -->
<!--   rename(mean = .estimate) %>%  -->
<!--   bind_rows(collect_metrics(rf_results) %>%  -->
<!--               mutate(set = "training")) %>%  -->
<!--   filter(str_detect(.metric, "accuracy|roc_auc")) %>%  -->
<!--   ggplot(aes(x= .metric, y=mean, color = set))+ -->
<!--   geom_point() +  -->
<!--   geom_errorbar(aes(xmin = mean - std_err, -->
<!--                     xmax = mean + std_err), -->
<!--                 width = 0.2, alpha = 0.5) -->


<!-- final_fit %>%  -->
<!--   collect_predictions() %>%  -->
<!--   conf_mat(truth = is_lipids, estimate = .pred_class) -->
<!-- final_fit %>%  -->
<!--   collect_predictions() %>%  -->
<!--   conf_mat(truth = is_lipids, estimate = .pred_class) %>%  -->
<!--   autoplot() -->
<!-- ############################## -->

<!-- # test_fit <- last_fit %>%  -->
<!-- #   predict(test_data) -->

<!-- predicted <- final_fit %>%  -->
<!--   collect_predictions() -->

<!-- test_data %>%  -->
<!--   rename(Original_is_lipids = is_lipids) %>%  -->
<!--   bind_cols(., predicted) %>%  -->
<!--   rename(predicted_is_lipids = .pred_class) %>%  -->
<!--   mutate(concordant = case_when( -->
<!--     Original_is_lipids == predicted_is_lipids      ~ "truly classified", -->
<!--     Original_is_lipids != predicted_is_lipids      ~ "wrongly classified", -->
<!--   )) %>%  -->
<!--   ggplot(aes(x=row_m_z, row_retention_time, color= concordant))+ -->
<!--   geom_point()+ -->
<!--   theme_minimal()+ -->
<!--   facet_wrap(. ~ Original_is_lipids, ncol = 2) -->
<!-- ``` -->
<!-- <br> -->

<!-- *** -->

<!-- ## Prediction on non identified metabolites : Lipids Yes / No (Ramdom forest) -->
<!-- ```{r fit on testing RF 2} -->
<!-- set.seed(101) -->

<!-- last_fit <- final_rf %>%  -->
<!--   fit(train_data) -->

<!-- test_fit <- last_fit %>%  -->
<!--   predict(unknown_metabolites) -->

<!-- predicted_metabolites <- unknown_metabolites %>%  -->
<!--   bind_cols(., test_fit) %>%  -->
<!--   rename(is_lipids = .pred_class) %>%  -->
<!--   mutate(pred = "prediction") %>%  -->
<!--   bind_rows(., train_data %>%   -->
<!--               mutate(pred = "measured")) -->


<!-- predicted_metabolites %>%  -->
<!--   ggplot(aes(x=row_m_z, row_retention_time, color= pred))+ -->
<!--   geom_point()+ -->
<!--   theme_minimal()+ -->
<!--   facet_wrap(. ~ is_lipids, ncol = 2) -->
<!-- ``` -->

<!-- ## Prediction on non identified metabolites : Lipids Yes / No (Logistic Regression) -->
<!-- The blue points are the unknown metabolites that we can predict to be lipids or not. We can see a pretty good ovelap with the orange point from the known metabolites dataset. -->
<!-- ```{r} -->
<!-- ############################################################################################### AT LAST -->
<!-- # Step finalize Fit : fitting to the whole training and evaluating on the testing data -->
<!-- # With the model of our choice -->
<!-- set.seed(101) -->

<!-- last_fit <- final_glmnet %>%  -->
<!--   fit(train_data) -->

<!-- test_fit <- last_fit %>%  -->
<!--   predict(unknown_metabolites) -->

<!-- predicted_metabolites <- unknown_metabolites %>%  -->
<!--   bind_cols(., test_fit) %>%  -->
<!--   rename(is_lipids = .pred_class) %>%  -->
<!--   mutate(pred = "prediction") %>%  -->
<!--   bind_rows(., mldata %>%   -->
<!--               mutate(pred = "measured")) -->


<!-- predicted_metabolites %>%  -->
<!--   ggplot(aes(x=row_m_z, row_retention_time, color= pred))+ -->
<!--   geom_point()+ -->
<!--   theme_minimal()+ -->
<!--   scale_color_manual(values = c("#0B0405FF", "#54C9ADFF"))+ -->
<!--   # scale_color_viridis_d(option = "G")+ -->
<!--   facet_wrap(. ~ is_lipids, ncol = 2) -->
<!-- ``` -->





