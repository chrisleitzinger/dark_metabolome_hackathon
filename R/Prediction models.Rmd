---
title: "Metabolites Prediction"
author: "Christelle Colin-Leitzinger"
date: '`r Sys.Date()`'
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: false
    theme: united
    highlight: pygments
    df_print: paged
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
.figure {
   margin-top: 25px;
   margin-bottom: 10px;
}

table {
    margin-top: 10px;
    margin-bottom: 25px;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      cache = FALSE,
                      fig.align='center'
                      )
```


```{r library}
library(tidyverse)
library(gtsummary)
# library(ComplexHeatmap)
library(corrplot)
library(ggcorrplot)
library(tidymodels)
library(themis)
library(discrim)
library(klaR)
library(rpart.plot)
library(vip) 
library(finalfit) 
theme_set(theme_classic())
```


```{r load}
# Samples, retention time and m/z data
omics_data <- read_delim(paste0(here::here(), "/new_process_iron_log2_merged.txt")) %>% 
  janitor::clean_names()
#
metabolites_data <- read_delim(paste0(here::here(), "/hmdb_keep_v4_python_blessed.txt.zip")) %>% 
  janitor::clean_names()
```

# **In the html, you can find the prediction of unknown metabolites using retention time and m/z information alone as predictors in part I and using Samples, retention time and m/z information as predictors in part II.**

<br>

***
<br>

# 1. Data cleaning

- The metabolite looking like `HMDB0000156|HMDB0031518` are removed.
```{r merge clean2 data}
omics_data <- omics_data %>% 
  # Filter 
  filter(!str_detect(hmdb, "\\|") | is.na(hmdb))
```

- Including meaningful data points  
In the raw data, we observe metabolites before 0.5 and after 14 for retention time (red rectangles). These metabolites are excluded to eliminate false detection during set up, washing and equilibrating the HPLC. The pics before aand after the blue lines were removed in second plot.
```{r}
omics_data %>% 
  ggplot(aes(x= row_retention_time, y=row_m_z
             ))+
  geom_point()+
  geom_rect(aes(xmin = 0, xmax = 0.5+0.3, ymin = min(row_m_z)-10, ymax = max(row_m_z) +2), 
            color= "red", fill="transparent") +
  geom_rect(aes(xmin = 14-0.2, xmax = max(row_retention_time)+0.3, ymin =  min(row_m_z)-10, ymax = max(row_m_z) +2),
            color= "red", fill="transparent")
```

```{r clean retention time}
clean_metabolites <- omics_data %>% 
  # cleaning retention time to eliminate false detection during set up, washing and equilibrating the HPLC
  filter(row_retention_time > 0.5 & row_retention_time < 14)

clean_metabolites %>% 
  mutate(row_retention_time = round(row_retention_time, 1)) %>% 
  ggplot(aes(x= row_retention_time))+
  geom_bar(stat = "count")+
  geom_vline(aes(xintercept= 0.5), color= "blue")+
  geom_vline(aes(xintercept= 14), color= "blue")+
  xlim(0, 20)+
  theme_classic()
```

- Patient samples present missing values :  
Not important for modeling without samples (Part 1) but I proceed with it now to work with an identical data in part 2 if we decide to mention quickly that we have done it.  
The data is imputed with the minimum value for each patient.
```{r}
clean_metabolites %>%
  dplyr::select(starts_with("t")) %>% 
  ff_glimpse()
```
```{r data prep1}
clean_metabolites <- clean_metabolites %>% 
  # Imputation samples data
  mutate(across((starts_with("t")), .fns = ~ replace_na(., min(., na.rm = TRUE))))
```

- Within this data, we have duplicated row_id (positive and negative).  
We have more metabolites negatively charged
```{r}
clean_metabolites %>%
  separate(row_id, into = c("charge", "id")) %>%
  arrange(id) %>% 
  mutate(id = as.character(id)) %>%
  ggplot(aes(x=charge, y=id, fill= charge))+
  geom_tile()+
  ggtitle("In overall metabolites")

clean_metabolites %>%
  separate(row_id, into = c("charge", "id")) %>%
  arrange(id) %>% 
  filter(non_heavy_identified_flag == 1) %>% 
  mutate(id = as.character(id)) %>%
  ggplot(aes(x=charge, y=id, fill= charge))+
  geom_tile()+
  ggtitle("In non_heavy_identified_flag == 1")
```
Select only 1 instance of a metabolite (keep negative over the positive as they are the more present). The positive form of a metabolite is kept in the data when the negative form is not present.
```{r clean data2}
# Need 2 steps
# select 1 in identified metabolites
clean_metabolites1 <- clean_metabolites %>%
  filter(non_heavy_identified_flag == 1) %>% 
  # remove same metabolite if present as neg and pos
  separate(row_id, into = c("charge", "id"), remove = FALSE) %>%
  arrange(id, charge) %>%
  distinct(id, .keep_all = TRUE)

# select 1 in non identified metabolites
clean_metabolites2 <- clean_metabolites %>%
  filter(non_heavy_identified_flag == 0) %>% 
  # remove same metabolite if present as neg and pos
  separate(row_id, into = c("charge", "id"), remove = FALSE) %>%
  arrange(id, charge) %>%
  distinct(id, .keep_all = TRUE)

# Bind
clean_metabolites <- bind_rows(clean_metabolites1, clean_metabolites2) %>% 
  distinct(id, .keep_all = TRUE)

rm(clean_metabolites1, clean_metabolites2)

clean_metabolites %>%
  # filter metabolites in positive or negative detection
  separate(row_id, into = c("charge", "id")) %>%
  filter(duplicated(id)) %>%
  full_join(., omics_data %>%
              separate(row_id, into = c("charge", "id")) %>%
              filter(!duplicated(id)),
            by="id") %>%
  dplyr::select(charge.x, id, charge.y) %>%
  filter(is.na(charge.x) | is.na(charge.y)) %>%
  pivot_longer(cols = c(charge.x, charge.y)) %>%
  filter(!is.na(value)) %>%
  ggplot(aes(x=value, fill= value))+
  geom_bar()
```
After cleaning, we have `r nrow(clean_metabolites)` unique metabolites in the data. `r nrow(clean_metabolites %>% filter(charge == "neg"))` are negatively charged. and `r nrow(clean_metabolites %>% filter(charge == "pos"))` are positively charged.


# 2. Data exploration
## Identified metabolites
```{r}
clean_metabolites %>% 
  filter(non_heavy_identified_flag == 0) %>% 
  ggplot(aes(x= row_retention_time, y=row_m_z))+
  geom_point(color= "yellow")+
  geom_point(data= omics_data %>% filter(non_heavy_identified_flag == 1),
             aes(x= row_retention_time, y=row_m_z), color= "red")+
  ggtitle("non_heavy_identified_flag == 1 are in red")

tbl <- clean_metabolites %>% 
  dplyr::select(non_heavy_identified_flag) %>% 
  mutate(non_heavy_identified_flag = case_when(
    non_heavy_identified_flag == 1            ~ "identified",
    TRUE                                      ~ "non identified"
  )) %>% 
  tbl_summary()
tbl
```
_We are trying to predict metabolite classifications. In our proteomic data, about `r inline_text(tbl, variable = non_heavy_identified_flag, level = "non identified", pattern = "{p}%")` of the metabolites are not identified. We want to see if we can predict with a good accuracy the classification of these metabolites based on **retention time and m/z** information (Part I). We also want to assess if using **patient samples data as predictors** in addition to theses 2 parameters can improves the model performance (Part II)._  

```{r merge data}
full_data <- clean_metabolites %>%
  # Join with known metabolites
  left_join(., metabolites_data, # %>%
              # dplyr::select(accession, taxonomy_super_class),
            by = c("hmdb" = "accession"))
```

## Summary table
```{r}
full_data %>% 
  filter(non_heavy_identified_flag == 1) %>% 
  dplyr::select(average_molecular_weight, monisotopic_molecular_weight, 
                taxonomy_class, taxonomy_direct_parent, 
                taxonomy_molecular_framework, taxonomy_sub_class,
                taxonomy_super_class) %>% 
  tbl_summary()
```

```{r is_lipids}
full_data <- full_data %>%
  mutate(is_lipids = case_when(
    str_detect(taxonomy_super_class, "Lipids")              ~ "Yes",
    TRUE                                                    ~ "No"
  )) %>%
  mutate_if(is.character, factor)
```

```{r unknown_metabolites}
unknown_metabolites <- full_data %>%
  # Filter identified metabolites
  filter(non_heavy_identified_flag != 1) %>%
  dplyr::select(row_id,
         row_m_z, row_retention_time,
         starts_with("t"),
         is_lipids)
```

```{r data prep}
two_predictor_data <- full_data %>%
  # Filter identified metabolites
  filter(non_heavy_identified_flag == 1) %>%
  dplyr::select(hmdb,
         row_m_z, row_retention_time,
         is_lipids)

sample_predictor_data <- full_data %>%
  # Filter identified metabolites
  filter(non_heavy_identified_flag == 1) %>%
  dplyr::select(hmdb,
         row_m_z, row_retention_time,
         starts_with("t"), -starts_with("taxonomy"),
         is_lipids)
```

## Is the data balanced? **No**
- We are focusing on predicting if a metabolite is a **lipid Yes/No**. I am using a variable I created, `is_lipids`, as the outcome.
```{r}
two_predictor_data %>%
  ggplot(aes(x=is_lipids, fill= is_lipids))+
  geom_bar()

two_predictor_data %>% 
  dplyr::select(is_lipids) %>% 
  tbl_summary(type = list(is_lipids ~ "categorical"))
```

<br>
<br>

***
<br>

# 3. Data preprocessing

## What are the steps to take that could improve the predictions?

Investigate if the data needs to be balanced and/or normalized, and/or remove correlated variable. We are going to look if and which preprocessing is useful to improve the models performance:  
1- Normalize numeric data to have a standard deviation of one with *scale* step or ;  
2- Normalize numeric data to have a standard deviation of one and a mean of zero with *normalize* step.  
3- Balanced the outcome variable with *smote* step. This use nearest neighbor to create new synthetic observation almost similar.  
4- Remove correlated variables which can decrease model performance with *corr* step.  

```{r}
set.seed(123)

# 1. Splitting the data
# 3/4 of the data into the training set but split evenly winthin race
data_split <- initial_split(two_predictor_data, prop = .75, strata = is_lipids)
# Create training and testing datasets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```
So I initialize multiple machine learning recipes including none of those steps, each of those steps alone and crossing steps with each other.
```{r recipe}
recipe_basic <-
  # 1.model formula
  recipe(is_lipids ~ ., data = train_data)  %>%
   # 2.keep these variables but not use them as either outcomes or predictors
  update_role(hmdb, new_role = "ID")

recipe_smote <-
  # 1.model formula
  recipe(is_lipids ~ ., data = train_data)  %>%
   # 2.keep these variables but not use them as either outcomes or predictors
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_smote(is_lipids)

recipe_scale <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors())

recipe_norm <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors())

recipe_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_corr(all_numeric_predictors())

recipe_scale_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_norm_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_corr_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_scale_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors())

recipe_norm_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors())

recipe_norm_smote_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_scale_smote_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)
```

```{r Generate List of recipes}
recipe_list <-
list(basic = recipe_basic,
     balanced = recipe_smote,
     scaled = recipe_scale, normalized = recipe_norm,
     corr.removed = recipe_corr,
     balanced.scaled = recipe_scale_smote, balanced.normalized = recipe_norm_smote,
     balanced.corr = recipe_corr_smote,
     scaled.corr = recipe_scale_corr, normalized.corr = recipe_norm_corr,
     balanced.normalized.corr = recipe_norm_smote_corr,
     balanced.scaled.corr = recipe_scale_smote_corr)
```

Multiple classification models will be evaluated in conjunction with these recipes :  
- bayesian model, decision tree and a trimmed decision tree, support-vector machine, Random forest, boosted tree, elasticnet, logistic regression (lasso, ridge), nearest neighbor.  
Each of these models/recipes will be evaluated on a the training dataset using a 10 fold cross validation approach.
```{r}
nb_spec <-
  naive_Bayes(smoothness = tune(), Laplace = tune()) %>%
  set_engine("klaR") %>%
  set_mode("classification")
```

```{r}
dt_spec <- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

n2_dt_spec <- decision_tree(cost_complexity = tune(), tree_depth = 2, min_n = 2) %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

```{r}
svm_spec <-
  svm_rbf(cost = tune(), rbf_sigma = tune(), margin = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")
```

```{r rand_forest}
ranger_spec <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger")
```

```{r xgboost}
xgboost_spec <-
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(),
             loss_reduction = tune(), sample_size = tune()) %>%
  set_mode("classification") %>%
  set_engine("xgboost")
# xgboost_workflow <-
#   workflow() %>%
#   add_recipe(mldata_recipe) %>%
#   add_model(xgboost_spec)
# set.seed(345)
# xgboost_tune <-
#   tune_grid(xgboost_workflow, resamples = mldata_folds, grid = 10)
```

```{r}
glmnet_spec <-
  logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

ridge_spec <-
  logistic_reg(penalty = tune(), mixture = 0) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

lasso_spec <-
  logistic_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")
# glmnet_workflow <-
#   workflow() %>%
#   add_recipe(mldata_recipe) %>%
#   add_model(glmnet_spec)
# glmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0, 0.05,
#                                                                                       0.2, 0.4, 0.6, 0.8, 1))
# set.seed(345)
# glmnet_tune <-
#   tune_grid(glmnet_workflow, resamples = mldata_folds, grid = glmnet_grid)
```

```{r}
knn_spec <-
  nearest_neighbor(neighbors = tune(), weight_func = tune()) %>%
  set_mode("classification") %>%
  set_engine("kknn")
# knn_workflow <-
#   workflow() %>%
#   add_recipe(mldata_recipe) %>%
#   add_model(knn_spec)
# set.seed(345)
# knn_tune <-
#   tune_grid(knn_workflow,
#             resamples = mldata_folds, 20)
```

```{r Generate list of models}
model_list <-
list(Random_Forest = ranger_spec, SVM = svm_spec, Naive_Bayes = nb_spec,
     Decision_Tree = dt_spec, n2_Decision_Tree = n2_dt_spec,
     Boosted_Trees = xgboost_spec, KNN = knn_spec, Elasticnet = glmnet_spec,
     Ridge_Regression = ridge_spec, Lasso_Regression = lasso_spec)
```

```{r model_set}
model_set <- workflow_set(preproc = recipe_list, models = model_list, cross = T)
```

All combinations of the preprocessors (recipes) and models are compared on their performance to choose the one which perform the best.  


## Machine learning data sets

The known metabolites data is split in to a training dataset (75%) and testing dataset (25%). The split is done evenly within the outcome variable `is_lipids`.
I use a **10 fold cross validation to train and evaluate model on the training set**. Resampling procedure will allow us to fit each workflow to the training data and compute accuracy on each resample. Then we compare the mean of the performance to pick the model/recipe cross combination which perform the best.

```{r cv fold}
set.seed(456)
mldata_folds <- vfold_cv(train_data, strata = is_lipids)
```


```{r run workflows}
doParallel::registerDoParallel()
set.seed(789)
all_workflows <-
  model_set %>% workflow_map(resamples = mldata_folds,
                             verbose = TRUE)
```

## 4. Visualize performance comparison of combined workflows - Explore Tuning param
```{r, fig.width=12, fig.height=12}
collect_metrics(all_workflows) %>%
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
  filter(.metric == "accuracy") %>%
  group_by(wflow_id) %>%
  filter(mean == max(mean)) %>%
  group_by(model) %>%
  dplyr::select(-.config) %>%
  distinct() %>%
  ungroup() %>%
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  mutate(Model_Type = factor(Model_Type,
                             levels = c("Decision_Tree", "n2_Decision_Tree", "Random_Forest",
                                        "Boosted_Trees",
                                        "Elasticnet", "Ridge_Regression", "Lasso_Regression", 
                                        "KNN", "SVM", "Naive_Bayes"
                                        ))) %>%
  filter(!is.na(Model_Type)) %>%
  mutate(Recipe = factor(Recipe,
                             levels = c("basic", "corr.removed", "normalized", "scaled", "balanced",
                                        "normalized.corr", "scaled.corr", "balanced.corr",
                                        "balanced.normalized", "balanced.scaled",
                                        "balanced.normalized.corr", "balanced.scaled.corr"
                                        ))) %>% arrange(desc(mean)) %>% 
  # mutate(mn = 0.912) %>% mutate(mx = 0.94) %>%
  ggplot(aes(x=Workflow_Rank, y = mean, shape = Recipe, color = Model_Type)) +
  # geom_rect(aes(xmin = -Inf, xmax = Inf,
  #           ymin = mn, ymax = mx),
  #           fill= "grey90", color= "transparent")+
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err)) +
  # geom_hline(aes(yintercept = 0.926), color= "darkgrey", linetype= 2)+
  # ylim(0.57, 0.95)+
  theme_classic()+
  scale_colour_manual(values=c("#1F78B4", "#FF7F00", "#B2DF8A", "#A6CEE3", "#FDBF6F",
                               "#33A02C", "#E31A1C", "#FB9A99", "#CAB2D6", "tomato"))+
  scale_shape_manual(values=c(0, 15, 1, 16, 2, 17, 8, 18, 3, 7, 25, 4))+
  labs(title = "Performance Comparison of Workflow Sets", x = "Workflow Rank",
       y = "Accuracy", color = "Models", shape = "Recipes")

performance_workflows <- collect_metrics(all_workflows) %>%
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
  filter(.metric == "accuracy") %>%
  group_by(wflow_id) %>%
  filter(mean == max(mean)) %>%
  group_by(model) %>%
  dplyr::select(-.config) %>%
  distinct() %>%
  ungroup() %>%
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  mutate(Model_Type = factor(Model_Type,
                             levels = c("Decision_Tree", "n2_Decision_Tree", "Random_Forest",
                                        "Boosted_Trees",
                                        "Elasticnet", "Ridge_Regression", "Lasso_Regression", 
                                        "KNN", "SVM", "Naive_Bayes"
                                        ))) %>%
  filter(!is.na(Model_Type)) %>%
  mutate(Recipe = factor(Recipe,
                             levels = c("basic", "corr.removed", "normalized", "scaled", "balanced",
                                        "normalized.corr", "scaled.corr", "balanced.corr",
                                        "balanced.normalized", "balanced.scaled",
                                        "balanced.normalized.corr", "balanced.scaled.corr"
                                        ))) %>% arrange(desc(mean))
two_predictor_perfmean <- performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "n2_Decision_Tree") %>% dplyr::select(mean)
two_predictor_perfsd <- performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "n2_Decision_Tree") %>% dplyr::select(std_err)
```
**Best model and recipe**
The best 2 models are 
- `r as.character(performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "Random_Forest") %>% dplyr::select(Model_Type))` with an accurracy of `r as.character(performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "Random_Forest") %>% dplyr::select(mean))`.  
- `r as.character(performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "Decision_Tree") %>% dplyr::select(Model_Type))` with an accuracy of `r as.character(performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "Decision_Tree") %>% dplyr::select(mean))`.  
<span style="color:green">Although random forest perform better, we decided to move forward with a simpler model which is the Trimmed decision tree. The measured accuracy is within the SD of the 2 models performing better.</span>  
- `r as.character(performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "n2_Decision_Tree") %>% dplyr::select(Model_Type))` with an accuracy of `r as.character(performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "n2_Decision_Tree") %>% dplyr::select(mean))`.   
```{r, fig.width=12, fig.height=6}
collect_metrics(all_workflows) %>%
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
  filter(.metric == "accuracy") %>%
  group_by(wflow_id) %>%
  filter(mean == max(mean)) %>%
  group_by(model) %>%
  dplyr::select(-.config) %>%
  distinct() %>%
  ungroup() %>%
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  mutate(Model_Type = factor(Model_Type,
                             levels = c("Decision_Tree", "n2_Decision_Tree", "Random_Forest"
                                        ))) %>%
  filter(!is.na(Model_Type)) %>%
  mutate(Recipe = factor(Recipe,
                             levels = c("basic", "corr.removed", "normalized", "scaled", "balanced",
                                        "normalized.corr", "scaled.corr", "balanced.corr",
                                        "balanced.normalized", "balanced.scaled",
                                        "balanced.normalized.corr", "balanced.scaled.corr"
                                        ))) %>%
  ggplot(aes(x=Recipe, y = mean, shape = Recipe, color = Model_Type)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err),
                width = 0.1) +
  geom_hline(aes(yintercept = 0.926), color= "darkgrey", linetype= 2)+
  theme_classic()+
  theme(axis.text.x = element_blank())+
  scale_colour_manual(values=c("#1F78B4", "#FF7F00", "#B2DF8A"))+
  scale_shape_manual(values=c(0, 15, 1, 16, 2, 17, 8, 18, 3, 7, 25, 4))+
  labs(title = "Performance Comparison of Workflow Sets", x = NULL,
       y = "Accuracy", color = "Model", shape = "Recipes")+
  guides(color = "none")+
  facet_wrap(. ~ Model_Type)
```
<span style="color:green">The best **accuracy** can be obtained with the basic recipe. Removing correlated variables, normalizing or scaling the data does not improve the performance. Balancing the data gives worse performance.</span>
<span style="color:green">I choose to continue with the recipe which change the less the data, **basic recipe**.</span>

```{r Extract best tunning param}
set.seed(789)

final_n2_tree <- all_workflows %>% 
  extract_workflow("basic_n2_Decision_Tree") %>% 
  finalize_workflow(all_workflows %>% 
  extract_workflow_set_result("basic_n2_Decision_Tree") %>% 
  select_best(metric = "accuracy"))

set.seed(789)

final_rf <- all_workflows %>% 
  extract_workflow("basic_Random_Forest") %>% 
  finalize_workflow(all_workflows %>% 
  extract_workflow_set_result("basic_Random_Forest") %>% 
  select_best(metric = "accuracy"))

set.seed(789)

final_knn <- all_workflows %>%
  extract_workflow("basic_KNN") %>%
  finalize_workflow(all_workflows %>%
  extract_workflow_set_result("basic_KNN") %>%
  select_best(metric = "accuracy"))
```

These are the hyperpareameters values used by the 2 best models (added the RF and knn just for reference).
```{r hyperpareameters}
final_n2_tree
final_rf
final_knn
```

## 5.Evaluate the 3 best models in depth on training set
### roc_auc, accuracy, sensitivity, specificity
```{r fit_resamples}
set.seed(789)
n2_tree_results <- final_n2_tree %>%
  fit_resamples(
    resamples = mldata_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
    control = control_resamples(save_pred = TRUE, verbose = TRUE)
  )
set.seed(789)
rf_results <- final_rf %>%
  fit_resamples(
    resamples = mldata_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
    control = control_resamples(save_pred = TRUE, verbose = TRUE)
  )
set.seed(789)
knn_results <- final_knn %>%
  fit_resamples(
    resamples = mldata_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
    control = control_resamples(save_pred = TRUE)
  )
```
<br>

```{r Performance Metrics}
n2_tree_results %>% # Compare both models
  collect_metrics() %>%
  dplyr::select(".metric", tree_mean = mean) %>%
  full_join(., rf_results %>% 
              collect_metrics() %>%
              dplyr::select(".metric", rf_mean = mean), by = ".metric") %>%
  full_join(., knn_results %>%
              collect_metrics() %>%
              dplyr::select(".metric", knn_mean = mean), by = ".metric")

n2_tree_results %>% # Compare both models
  collect_metrics() %>%
  mutate(model = "tree") %>%
  bind_rows(rf_results %>%
              collect_metrics() %>%
              mutate(model = "rf")) %>%
  bind_rows(knn_results %>%
              collect_metrics() %>%
              mutate(model = "knn")) %>%

  ggplot(aes(x= .metric, y=mean, fill = model))+
  geom_bar(stat = "identity",
           position = position_dodge())+
  geom_errorbar(aes(x = .metric,
                    ymin = mean - std_err,
                    ymax = mean + std_err),
                width = 0.2, alpha = 0.5,
                position=position_dodge(width=0.9))+
  theme_classic()+
  scale_fill_viridis_d(option = "A", begin= 0.2, end = 0.8)
```
<span style="color:green">roc-auc and accuracy show a really good performance. The sensitivity (ability to detect a true positive) is good but both models suffer from small number of lipid observation in the data which leads to a lower specificity (No lipids are classified as lipids).</span>

While ROC shows how the TPR and FPR vary with the threshold, the ROC AUC is a measure of the classification model's ability to distinguish one class from the other.

### PR

PR curves are sensitive to which class is defined as positive, unlike the ROC curve which is a more balanced method. So we will get a completely different result depending on which class is set as positive.
PR curves are sensitive to class distribution, so if the ratio of positives to negatives changes across different analyses, then the PR curve cannot be used to compare performance between them. This is because the chance line varies between datasets with different class distributions.
PR curves, because they use precision, instead of specificity (like ROC) can pick up false positives in the predicted positive fraction. This is very helpful when negatives >> positives. In these cases, the ROC is pretty insensitive and can be misleading, whereas PR curves reign supreme.

```{r evaluate models}
n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  bind_rows(knn_results %>%
              collect_predictions() %>%
              mutate(model = "knn")) %>%
  
# rf_results %>% # Compare both models
#   collect_predictions() %>%
#   mutate(model = "rf") %>%
#   bind_rows(knn_results %>%
#               collect_predictions() %>%
#               mutate(model = "glmnet")) %>%
  group_by(model) %>%
  roc_curve(is_lipids, .pred_No) %>%
  autoplot()+
  ggtitle("Area under Curve ROC (Receiver Operating Characteristics)")
n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  bind_rows(knn_results %>%
              collect_predictions() %>%
              mutate(model = "knn")) %>%
  group_by(model) %>%
  roc_auc(is_lipids, .pred_No)
```
<span style="color:green">The roc-auc curve and value are very similar between the 2 models.</span>

Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced.
precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.

The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.

```{r}
n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  bind_rows(knn_results %>%
              collect_predictions() %>%
              mutate(model = "knn")) %>%
  group_by(model) %>%
  pr_curve(is_lipids, .pred_Yes) %>%
  autoplot()+
  ggtitle("Area under the precision recall curve - \nYes prediction")
n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  bind_rows(knn_results %>%
              collect_predictions() %>%
              mutate(model = "knn")) %>%
  group_by(model) %>%
  average_precision(is_lipids, .pred_Yes)

n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  bind_rows(knn_results %>%
              collect_predictions() %>%
              mutate(model = "knn")) %>%
  group_by(model) %>%
  pr_curve(is_lipids, .pred_No) %>%
  autoplot()+
  ggtitle("Area under the precision recall curve - \nNo prediction")
n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  bind_rows(knn_results %>%
              collect_predictions() %>%
              mutate(model = "knn")) %>%
  group_by(model) %>%
  average_precision(is_lipids, .pred_No)
```
<span style="color:green">I never used PR measurement so I would love to hear your input.</span>
For what I can see here, the prediction for No is very precise when the prediction for Yes is less precise:
<span style="color:green">For RF, high recall but low precision : returns many results, but 32% of the lipids Yes prediction are incorrect. Only 2.5% are predicted wrongly as lipids No.</span>
<br>
<br>


# Fit on the whole training / predict on testing data
## Analyse results
```{r}
set.seed(789)

# final_fit <- all_workflows %>% 
#   extract_workflow("basic_Random_Forest") %>% 
#   finalize_workflow(all_workflows %>% 
#                       extract_workflow_set_result("basic_Random_Forest") %>% 
#                       select_best(metric = "accuracy")) %>% 
#   fit(train_data)


final_fit <- all_workflows %>% 
  extract_workflow("basic_n2_Decision_Tree") %>% 
  finalize_workflow(all_workflows %>% 
                      extract_workflow_set_result("basic_n2_Decision_Tree") %>% 
                      select_best(metric = "accuracy")) %>% 
  last_fit(data_split)
```
## 1.Look at performance with collect_metrics on the testing data (can see if over fit)
```{r}
collect_metrics(final_fit)
```
As a reminder, performance on the training set was:
```{r}
# Compare to the training previous number
collect_metrics(n2_tree_results)[1:2,]
```
We can see we might be slightly overfitting.
```{r}
collect_metrics(final_fit) %>% 
  `colnames<-`(c("metric", "estimator", "performance", "config")) %>% 
  mutate(model = "testing") %>% 
  bind_rows(.,
            collect_metrics(n2_tree_results)[1:2,] %>% 
              `colnames<-`(c("metric", "estimator", "performance", "n", "std_err", "config")) %>% 
              mutate(model = "training")
              ) %>% 
  ggplot(aes(x= metric, y=performance, fill = model))+
  geom_bar(stat = "identity",
           position = position_dodge())+
  geom_errorbar(aes(x = metric,
                    ymin = performance - std_err,
                    ymax = performance + std_err),
                width = 0.2, alpha = 0.5,
                position=position_dodge(width=0.9))+
  scale_fill_viridis_d(option = "A", begin= 0.2, end = 0.8)
```
## 2.Get predictions out on the testing dataset
```{r}
df <- final_fit %>%
  collect_predictions()
conf_tab <- table(df$is_lipids, df$.pred_class)
conf_tab <- conf_tab / rowSums(conf_tab)
conf_tab <- as.data.frame(conf_tab, stringsAsFactors = TRUE)
conf_tab$Var2 <- factor(conf_tab$Var2, rev(levels(conf_tab$Var2)))

ggplot(conf_tab, aes(Var1, Var2, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = scales::percent(Freq))) +
  scale_fill_gradient(low = "white", high = "#3575b5")+
  labs(x = "Truth", y = "Prediction", title = "Confusion matrix using Trimmed Decision Tree",
       fill = "Valid prediction")+
  theme_classic()

final_fit %>%
  collect_predictions() %>%
  conf_mat(is_lipids, .pred_class)
```
## 3.Look at variable importance
```{r}
dt_fit <- 
  final_fit %>% 
  extract_fit_parsnip()
#Generate Decision Tree Plot Using rpart.plot package
rpart.plot::rpart.plot(dt_fit$fit)

```

## 4.What is the m/z-rentention time pattern of predicted data over training data?
```{r}
training(data_split) %>% 
  mutate(data = "training") %>% 
  bind_rows(testing(data_split) %>% 
              mutate(data = "testing")) %>% 
  ggplot(aes(x=row_m_z, row_retention_time, color= data))+
  geom_point()+
  scale_color_discrete(name = NULL)+
  facet_wrap(. ~ is_lipids, ncol = 2)
```
What is the m/z-rentention time pattern of predicted data over real data?
```{r}
final_fit %>%
  collect_predictions() %>% dplyr::select(-is_lipids) %>% 
  bind_cols(testing(data_split)) %>% 
  mutate(concordant = case_when(
    is_lipids == .pred_class      ~ "truly classified",
    is_lipids != .pred_class      ~ "wrongly classified",
  )) %>% 
  ggplot(aes(x=row_m_z, row_retention_time, color= concordant))+
  geom_point()+
  scale_color_discrete(name = NULL)+
  facet_wrap(. ~ is_lipids, ncol = 2)
```

# Prediction on new data with Trimmed Decision Tree
The new data is cleaned in the same manner as the data used to create the machine learning model.  
- keep row_retention_time > 0.5 & < 14  
- keep only metabolite row entry when positive and negative metabolites are present
```{r prep new data}
new_data <- read_csv(paste0(here::here(), "/CICPT_1799_metabolomics_iron_log2_intensity.csv")) %>% 
  janitor::clean_names()
new_data <- new_data %>% 
  # cleaning retention time to eliminate false detection during set up, washing and equilibrating the HPLC
  filter(row_retention_time > 0.5 & row_retention_time < 14)

clean_metabolites1 <- new_data %>%
  filter(non_heavy_identified_flag == 1) %>% 
  # remove same metabolite if present as neg and pos
  separate(row_id, into = c("charge", "id"), remove = FALSE) %>%
  arrange(id, charge) %>%
  distinct(id, .keep_all = TRUE)

clean_metabolites2 <- new_data %>%
  filter(non_heavy_identified_flag == 0) %>% 
  # remove same metabolite if present as neg and pos
  separate(row_id, into = c("charge", "id"), remove = FALSE) %>%
  arrange(id, charge) %>%
  distinct(id, .keep_all = TRUE)

# Bind
new_data <- bind_rows(clean_metabolites1, clean_metabolites2) %>% 
  distinct(id, .keep_all = TRUE)
new_data <- new_data %>%
  # Join with known metabolites
  left_join(., metabolites_data, # %>%
            # dplyr::select(accession, taxonomy_super_class),
            by = c("hmdb" = "accession"))
new_data <- new_data %>%
  filter(non_heavy_identified_flag == 1) %>%
  mutate(is_lipids = case_when(
    str_detect(taxonomy_super_class, "Lipids")              ~ "Yes",
    TRUE                                                    ~ "No"
  )) %>%
  mutate_if(is.character, factor)
```


```{r predict new data}
final_fitted <- extract_workflow(final_fit)

set.seed(1234)
predicted_new_data <- augment(final_fitted, new_data)
```

get predictions out collect_predictions (will be on the testing) (make plot, confusion matrix)
```{r}
# df <- final_fit %>%
#   collect_predictions()
conf_tab <- table(predicted_new_data$is_lipids, predicted_new_data$.pred_class)
conf_tab <- conf_tab / rowSums(conf_tab)
conf_tab <- as.data.frame(conf_tab, stringsAsFactors = TRUE)
conf_tab$Var2 <- factor(conf_tab$Var2, rev(levels(conf_tab$Var2)))

ggplot(conf_tab, aes(Var1, Var2, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = scales::percent(Freq))) +
  scale_fill_gradient(low = "white", high = "#3575b5")+
  labs(x = "Truth", y = "Prediction", title = "Confusion matrix using Trimmed Decision Tree",
       fill = "Valid prediction")+
  theme_classic()

predicted_new_data %>%
  conf_mat(is_lipids, .pred_class)
```
look at var importance
```{r}
dt_fit <- 
  final_fitted %>% 
  extract_fit_parsnip()
#Generate Decision Tree Plot Using rpart.plot package
rpart.plot::rpart.plot(dt_fit$fit)
```

What is the m/z-rentention time pattern of predicted data?
```{r}
predicted_new_data %>% 
  mutate(concordant = case_when(
    is_lipids == .pred_class      ~ "truly classified",
    is_lipids != .pred_class      ~ "wrongly classified",
  )) %>% 
  ggplot(aes(x=row_m_z, row_retention_time, color= concordant))+
  geom_point()+
  scale_color_discrete(name = NULL)+
  facet_wrap(. ~ is_lipids, ncol = 2)
```


# Part II
**Here we are looking at if we increase the prediction of unknown metabolites using the Samples data on top of retention time and m/z information as predictors in part II.**

# Data modeling
```{r}
set.seed(123)

# 1. Splitting the data
# 3/4 of the data into the training set but split evenly winthin race
data_split <- initial_split(sample_predictor_data, prop = .75, strata = is_lipids)
# Create training and testing datasets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

### Confirm that the basic recipe gives the best performance and
```{r recipe part2}
recipe_basic <-
  # 1.model formula
  recipe(is_lipids ~ ., data = train_data)  %>%
   # 2.keep these variables but not use them as either outcomes or predictors
  update_role(hmdb, new_role = "ID")

recipe_smote <-
  # 1.model formula
  recipe(is_lipids ~ ., data = train_data)  %>%
   # 2.keep these variables but not use them as either outcomes or predictors
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_smote(is_lipids)

recipe_scale <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors())

recipe_norm <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors())

recipe_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_corr(all_numeric_predictors())

recipe_scale_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_norm_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_corr_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_scale_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors())

recipe_norm_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors())

recipe_norm_smote_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_scale_smote_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)
```

```{r Generate List of recipes part2}
recipe_list <-
list(basic = recipe_basic,
     balanced = recipe_smote,
     scaled = recipe_scale, normalized = recipe_norm,
     corr.removed = recipe_corr,
     balanced.scaled = recipe_scale_smote, balanced.normalized = recipe_norm_smote,
     balanced.corr = recipe_corr_smote,
     scaled.corr = recipe_scale_corr, normalized.corr = recipe_norm_corr,
     balanced.normalized.corr = recipe_norm_smote_corr,
     balanced.scaled.corr = recipe_scale_smote_corr)
```

```{r}
# nb_spec <-
#   naive_Bayes(smoothness = tune(), Laplace = tune()) %>%
#   set_engine("klaR") %>%#   ```{r}
```

```{r}
# dt_spec <- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>%
#   set_engine("rpart") %>% 
#   set_mode("classification")

n2_dt_spec <- decision_tree(cost_complexity = tune(), tree_depth = 2, min_n = 2) %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

<!-- ```{r} -->
<!-- svm_spec <- -->
<!--   svm_rbf(cost = tune(), rbf_sigma = tune(), margin = tune()) %>% -->
<!--   set_engine("kernlab") %>% -->
<!--   set_mode("classification") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ranger_spec <- rand_forest( -->
<!--   mtry = tune(), -->
<!--   min_n = tune(), -->
<!--   trees = tune()) %>% -->
<!--   set_mode("classification") %>% -->
<!--   set_engine("ranger") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- xgboost_spec <- -->
<!--   boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), -->
<!--              loss_reduction = tune(), sample_size = tune()) %>% -->
<!--   set_mode("classification") %>% -->
<!--   set_engine("xgboost") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- glmnet_spec <- -->
<!--   logistic_reg(penalty = tune(), mixture = tune()) %>% -->
<!--   set_mode("classification") %>% -->
<!--   set_engine("glmnet") -->

<!-- ridge_spec <- -->
<!--   logistic_reg(penalty = tune(), mixture = 0) %>% -->
<!--   set_mode("classification") %>% -->
<!--   set_engine("glmnet") -->

<!-- lasso_spec <- -->
<!--   logistic_reg(penalty = tune(), mixture = 1) %>% -->
<!--   set_mode("classification") %>% -->
<!--   set_engine("glmnet") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- knn_spec <- -->
<!--   nearest_neighbor(neighbors = tune(), weight_func = tune()) %>% -->
<!--   set_mode("classification") %>% -->
<!--   set_engine("kknn") -->
<!-- ``` -->

```{r Generate list of models part2}
model_list <-
list(# Random_Forest = ranger_spec, SVM = svm_spec, Naive_Bayes = nb_spec,
     # Decision_Tree = dt_spec, 
     n2_Decision_Tree = n2_dt_spec#,
     # Boosted_Trees = xgboost_spec, KNN = knn_spec, Elasticnet = glmnet_spec,
     # Ridge_Regression = ridge_spec, Lasso_Regression = lasso_spec
     )
```

```{r model_set part2}
model_set <- workflow_set(preproc = recipe_list, models = model_list, cross = T)
```

```{r cv fold part2}
set.seed(456)
mldata_folds <- vfold_cv(train_data, strata = is_lipids)
```


```{r run workflows part2}
doParallel::registerDoParallel()
set.seed(789)
all_workflows <-
  model_set %>% workflow_map(resamples = mldata_folds,
                             verbose = TRUE)
```

### Compare performance dataset with Samples info
```{r, fig.width=12, fig.height=6}
collect_metrics(all_workflows) %>%
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
  filter(.metric == "accuracy") %>%
  group_by(wflow_id) %>%
  filter(mean == max(mean)) %>%
  group_by(model) %>%
  dplyr::select(-.config) %>%
  distinct() %>%
  ungroup() %>%
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  filter(!is.na(Model_Type)) %>%
  mutate(Recipe = factor(Recipe,
                             levels = c("basic", "corr.removed", "normalized", "scaled", "balanced",
                                        "normalized.corr", "scaled.corr", "balanced.corr",
                                        "balanced.normalized", "balanced.scaled",
                                        "balanced.normalized.corr", "balanced.scaled.corr"
                                        ))) %>% arrange(desc(mean)) %>% 
  # mutate(mn = 0.912) %>% mutate(mx = 0.94) %>%
  ggplot(aes(x=Workflow_Rank, y = mean, shape = Recipe)) +
  geom_rect(aes(xmin = -Inf, xmax = Inf,
                ymin = as.numeric(two_predictor_perfmean) - as.numeric(two_predictor_perfsd),
                ymax = as.numeric(two_predictor_perfmean) + as.numeric(two_predictor_perfsd)),
            fill= "grey90", color= "transparent")+
  geom_point(size = 3, color= "#1F78B4") +
  geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err), color= "#1F78B4") +
  geom_hline(aes(yintercept = as.numeric(two_predictor_perfmean)), color= "darkgrey", linetype= 2)+
  # ylim(0.57, 0.95)+
  theme_classic()+
  scale_shape_manual(values=c(0, 15, 1, 16, 2, 17, 8, 18, 3, 7, 25, 4))+
  labs(title = "Performance Comparison of Workflow Sets", x = "Workflow Rank",
       y = "Accuracy", color = "Models", shape = "Recipes")

performance_workflows <- collect_metrics(all_workflows) %>%
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
  filter(.metric == "accuracy") %>%
  group_by(wflow_id) %>%
  filter(mean == max(mean)) %>%
  group_by(model) %>%
  dplyr::select(-.config) %>%
  distinct() %>%
  ungroup() %>%
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  filter(!is.na(Model_Type)) %>%
  mutate(Recipe = factor(Recipe,
                             levels = c("basic", "corr.removed", "normalized", "scaled", "balanced",
                                        "normalized.corr", "scaled.corr", "balanced.corr",
                                        "balanced.normalized", "balanced.scaled",
                                        "balanced.normalized.corr", "balanced.scaled.corr"
                                        ))) %>% arrange(desc(mean))
```
**Adding the samples as predictors with the Trimmed decision tree model** has an accuracy of `r as.character(performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "n2_Decision_Tree") %>% dplyr::select(mean))`.
As a reminder, the same model trained on the dataset containing m/z and retention time only had an average accuracy of `r as.character(two_predictor_perfmean)`+/- `r as.character(two_predictor_perfsd)` showed in the grey dotted line and rectangle.  
Adding the samples as predictors gives a similar performance of the Trimmed decision tree model.



