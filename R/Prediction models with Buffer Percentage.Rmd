---
title: "Metabolites Prediction using Buffer Percentage"
author: "Christelle Colin-Leitzinger"
date: '`r Sys.Date()`'
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: false
    theme: united
    highlight: pygments
    df_print: paged
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
.figure {
   margin-top: 25px;
   margin-bottom: 10px;
}

table {
    margin-top: 10px;
    margin-bottom: 25px;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      cache = FALSE,
                      fig.align='center'
                      )
```


```{r library}
library(tidyverse)
library(gtsummary)
# library(ComplexHeatmap)
library(corrplot)
library(ggcorrplot)
library(tidymodels)
library(themis)
library(discrim)
library(klaR)
library(rpart.plot)
library(vip) 
library(finalfit) 
theme_set(theme_classic())
theme_gtsummary_compact()
```


```{r load data}
# classification data
metabolites_classification <- 
  read_delim(paste0(here::here(), "/hmdb_keep_v4_python_blessed.txt.zip")) %>% 
  janitor::clean_names()

# metabolites expression data
omics1799_data <- 
  read_delim(
    paste0(
      here::here(), 
      "/flores_1799_tap73_metabolomics_reanalysis_2023-04-03/flores_1799_tap73_metabolomics_reanalysis_2023_iron_log2_merged.txt")) %>% 
  janitor::clean_names()
#
validation3990_data <- read_csv(paste0(here::here(), "/CICPT_3990_tissue_metabolomics_iron_log2_merged_edited.csv")) %>% 
  janitor::clean_names()
```

## **In the html, you can find the prediction of unknown metabolites using buffer_percent and m/z information alone as predictors in part I and using Samples, buffer_percent and m/z information as predictors in part II (data not shown?).**

<br>

***
<br>

# Part I
# 1. Data cleaning

We are using 1799 data to build the ML model and 3990 data is for validation.  

## Including meaningful data points - eliminate false detection
In the raw data, we observe metabolites before 1 and after 14 for row_retention_time (red rectangles). These metabolites are excluded to eliminate false detection during set up, washing and equilibrating the HPLC. As you can see in the histogram, the pics before and after the blue lines were removed.
<div class = "row">
<div class = "col-md-6">
#### Before cleaning
```{r}
omics1799_data %>% 
  ggplot(aes(x= row_retention_time, y=row_m_z
             ))+
  geom_point(size= 1)+
  geom_rect(aes(xmin = 0, xmax = 1+0.3, ymin = min(row_m_z)-10, ymax = max(row_m_z) +2), 
            color= "red", fill="transparent") +
  geom_rect(aes(xmin = 14-0.2, xmax = max(row_retention_time)+0.3, ymin =  min(row_m_z)-10, ymax = max(row_m_z) +2),
            color= "red", fill="transparent")
```
</div>

<div class = "col-md-6">
#### After cleaning
```{r clean retention time}
clean_metabolites <- omics1799_data %>% 
  # cleaning retention time to eliminate false detection during set up, washing and equilibrating the HPLC
  filter(row_retention_time > 1 & row_retention_time < 14)

validation3990_data <- validation3990_data %>% 
  filter(row_retention_time > 1 & row_retention_time < 14)

clean_metabolites %>% 
  mutate(row_retention_time = round(row_retention_time, 1)) %>% 
  ggplot(aes(x= row_retention_time))+
  geom_bar(stat = "count")+
  geom_vline(aes(xintercept= 1), color= "blue")+
  geom_vline(aes(xintercept= 14), color= "blue")+
  xlim(0, 15)+
  theme_classic()

clean_metabolites %>% 
  ggplot(aes(x= row_retention_time, y=row_m_z
             ))+
  geom_point(size= 1)+
  xlim(0, 15)
```
</div>
</div>

- Patient samples present missing values :  
Not important for modeling without samples (Part 1) but I proceed with it now to work with an identical data in part 2 if we decide to mention quickly that we have done it.  
The data is imputed with the minimum value for each patient.
```{r}
# clean_metabolites %>%
#   dplyr::select(starts_with("t")) %>% 
#   ff_glimpse()
```
```{r data prep1}
clean_metabolites <- clean_metabolites %>% 
  # Imputation samples data
  mutate(across((starts_with("flo")), .fns = ~ replace_na(., min(., na.rm = TRUE))))
```

## Including meaningful data points - eliminate duplicate
- Within this data, we have duplicated row_id (positive and negative).  
We have more metabolites negatively charged
```{r}
clean_metabolites %>%
  separate(row_id, into = c("charge", "id")) %>%
  ggplot(aes(x=charge, fill= charge))+
  geom_bar()+
  ggtitle("In overall metabolites")

clean_metabolites %>%
  filter(non_heavy_identified_flag == 1) %>% 
  separate(row_id, into = c("charge", "id")) %>%
  ggplot(aes(x=charge, fill= charge))+
  geom_bar()+
  ggtitle("In non_heavy_identified_flag == 1")
```

Select only 1 instance of a metabolite (keep negative over the positive as they are the more present). The positive form of a metabolite is kept in the data when the negative form is not present.
```{r clean data2}
# Need 2 steps
# select 1 in identified metabolites
clean_metabolites1 <- clean_metabolites %>%
  filter(non_heavy_identified_flag == 1) %>% 
  # remove same metabolite if present as neg and pos
  separate(row_id, into = c("charge", "id"), remove = FALSE) %>%
  arrange(id, charge) %>%
  distinct(id, .keep_all = TRUE)

# select 1 in non identified metabolites
clean_metabolites2 <- clean_metabolites %>%
  filter(non_heavy_identified_flag == 0) %>% 
  # remove same metabolite if present as neg and pos
  separate(row_id, into = c("charge", "id"), remove = FALSE) %>%
  arrange(id, charge) %>%
  distinct(id, .keep_all = TRUE)

# Bind
clean_metabolites <- bind_rows(clean_metabolites1, clean_metabolites2) %>% 
  distinct(id, .keep_all = TRUE)


clean_metabolites %>%
  # filter metabolites in positive or negative detection
  separate(row_id, into = c("charge", "id")) %>%
  filter(duplicated(id)) %>%
  full_join(., omics1799_data %>%
              separate(row_id, into = c("charge", "id")) %>%
              filter(!duplicated(id)),
            by="id") %>%
  dplyr::select(charge.x, id, charge.y) %>%
  filter(is.na(charge.x) | is.na(charge.y)) %>%
  pivot_longer(cols = c(charge.x, charge.y)) %>%
  filter(!is.na(value)) %>%
  ggplot(aes(x=value, fill= value))+
  geom_bar()+
  ggtitle("In non_heavy_identified_flag == 1")

# Do the same for the validation data
clean_metabolites1 <- validation3990_data %>%
  filter(non_heavy_identified_flag == 1) %>% 
  # remove same metabolite if present as neg and pos
  separate(row_id, into = c("charge", "id"), remove = FALSE) %>%
  arrange(id, charge) %>%
  distinct(id, .keep_all = TRUE)

clean_metabolites2 <- validation3990_data %>%
  filter(non_heavy_identified_flag == 0) %>% 
  # remove same metabolite if present as neg and pos
  separate(row_id, into = c("charge", "id"), remove = FALSE) %>%
  arrange(id, charge) %>%
  distinct(id, .keep_all = TRUE)

# Bind
validation3990_data <- bind_rows(clean_metabolites1, clean_metabolites2) %>% 
  distinct(id, .keep_all = TRUE)

rm(clean_metabolites1, clean_metabolites2)
```
After cleaning, we have `r nrow(clean_metabolites)` unique metabolites in the data. `r nrow(clean_metabolites %>% filter(charge == "neg"))` are negatively charged. and `r nrow(clean_metabolites %>% filter(charge == "pos"))` are positively charged.


## Add taxonomy to our data and create the dichotomous lipids variable Yes/No 
```{r merge data}
full_data <- clean_metabolites %>%
  # Join with known metabolites
  mutate(hmdb = str_remove(hmdb, "^\\|")) %>% 
  separate_wider_delim(cols = hmdb, delim = "|",
                       names = c("hmdb"), 
                       too_few = "align_start", too_many = "drop", 
                       cols_remove = TRUE) %>% 
  left_join(., metabolites_classification,
            by = c("hmdb" = "accession"))

  # left_join(., metabolites_classification,
  #           by = c("hmdb" = "accession"))

validation3990_data <- validation3990_data %>%
  mutate(hmdb = str_remove(hmdb, "^\\|")) %>% 
  separate_wider_delim(cols = hmdb, delim = "|",
                       names = c("hmdb"), 
                       too_few = "align_start", too_many = "drop", 
                       cols_remove = TRUE) %>% 
  left_join(., metabolites_classification,
            by = c("hmdb" = "accession"))
```

```{r is_lipids}
full_data <- full_data %>%
  mutate(is_lipids = case_when(
    row_id == "neg_00049"                                   ~ "No",
    str_detect(taxonomy_super_class, "Lipids")              ~ "Yes",
    TRUE                                                    ~ "No"
  )) %>%
  mutate_if(is.character, factor)

validation3990_data <- validation3990_data %>%
  filter(non_heavy_identified_flag == 1) %>% # Can already do this step here
  mutate(is_lipids = case_when(
    str_detect(taxonomy_super_class, "Lipids")              ~ "Yes",
    TRUE                                                    ~ "No"
  )) %>%
  mutate_if(is.character, factor)
```

```{r buffer concentrations}
# I have a function to figure out the buffer concentrations for you. You would just plug in the retention time x into
#  
# y1 = 4.615x + 20 for x >=0 and x =< 13
#  
# Since y2 is completely dependent on y1, for the sake of modeling maybe we do not need to include it?
 
# Did Dalia get you the data with the updated annotation? For the metabolites that had multiple mappings, she was going to note which ones were “all lipids” or “all not lipids” so they can be included in the modeling results instead of being skipped.
# 
# Oops and to be clear that “y1” would be a percentage not a concentration. Of course given the starting concentration we could use figure out the A and B concentrations, but again it is all linear equations so I am thinking it does not really matter if we use % vs mM.
```

## Calculate buffer concentration based on retention time
For a retention time from 0 to 13 (note that we don't have data point under 1 in our clean data) :
$$buffer\_percent = 4.615 * retention\_time + 20$$

```{r add buffer calculation}
full_data <- full_data %>%
  mutate(buffer_percent = case_when(
    row_retention_time >= 0 &
      row_retention_time <= 13            ~  4.615 * row_retention_time + 20
  ))

validation3990_data <- validation3990_data %>%
  mutate(buffer_percent = case_when(
    row_retention_time >= 0 &
      row_retention_time <= 13            ~  4.615 * row_retention_time + 20
  ))
```

# 2. Data exploration
## Identified metabolites
Even if we can see some big aggregate of metabolite around the min and max of retention time, most of those metabolites are unknown so will not be used in our modeling
```{r}
full_data %>% 
  filter(non_heavy_identified_flag == 0) %>% 
  ggplot(aes(x= row_retention_time, y=row_m_z))+
  geom_point(color= "yellow")+
  geom_point(data= omics1799_data %>% filter(non_heavy_identified_flag == 1),
             aes(x= row_retention_time, y=row_m_z), color= "red")+
  ggtitle("non_heavy_identified_flag == 1 are in red")
```

<div class = "row">
<div class = "col-md-6">
```{r}
tbl <- full_data %>% 
  dplyr::select(non_heavy_identified_flag) %>% 
  mutate(non_heavy_identified_flag = case_when(
    non_heavy_identified_flag == 1            ~ "identified",
    TRUE                                      ~ "non identified"
  )) %>% 
  tbl_summary()
tbl
```
</div>

<div class = "col-md-6">
```{r}
a <- full_data %>% 
  dplyr::select(non_heavy_identified_flag) %>% 
  mutate(non_heavy_identified_flag = case_when(
    non_heavy_identified_flag == 1            ~ "identified",
    TRUE                                      ~ "non identified"
  )) %>% 
  mutate(non_heavy_identified_flag = factor(non_heavy_identified_flag, 
                                            levels = c("non identified",
                                                       "identified"))) %>% 
  group_by(non_heavy_identified_flag) %>% 
  mutate(ypos = n()) %>% 
  mutate(ypos = ypos - 0.5*ypos) %>% 
  ungroup()

a %>% 
  ggplot(aes(x= "", fill= non_heavy_identified_flag))+
  geom_bar(width=1, color= "white") +
  coord_polar("y", start=0)+
  theme_void()+
  
  geom_text(data = a %>%
              distinct(non_heavy_identified_flag, .keep_all = TRUE),
            mapping = aes(x=1.6, y= ypos, label = non_heavy_identified_flag
            ))
```
</div>
</div>

_We are trying to predict the lipids classifications of metabolite. In our proteomic data, about `r inline_text(tbl, variable = non_heavy_identified_flag, level = "non identified", pattern = "{p}%")` of the metabolites are not identified. We want to see if we can predict with a good accuracy the classification of these metabolites based on **the buffer percent and m/z** information (Part I). We also want to assess if using **patient samples data as predictors** in addition to theses 2 parameters can improves the model performance (Part II)._  

## Summary table
```{r}
full_data %>% 
  filter(non_heavy_identified_flag == 1) %>% 
  dplyr::select(buffer_percent, 
                average_molecular_weight, monisotopic_molecular_weight, 
                taxonomy_class, taxonomy_direct_parent, 
                taxonomy_molecular_framework, taxonomy_sub_class,
                taxonomy_super_class) %>% 
  tbl_summary(sort = list(everything() ~ "frequency")) %>% 
  modify_header(
    label = "**Known Metabolites Characteristics in 1799 data**"
  ) %>% 
  bold_labels()
```

```{r unknown_metabolites}
unknown_metabolites <- full_data %>%
  # Filter identified metabolites
  filter(non_heavy_identified_flag != 1) %>%
  dplyr::select(row_id,
         row_m_z, buffer_percent,
         starts_with("t"),
         is_lipids)
```

```{r data prep}
two_predictor_data <- full_data %>%
  # Filter identified metabolites
  filter(non_heavy_identified_flag == 1) %>%
  dplyr::select(hmdb,
         row_m_z, buffer_percent,
         is_lipids)

sample_predictor_data <- full_data %>%
  # Filter identified metabolites
  filter(non_heavy_identified_flag == 1) %>%
  dplyr::select(hmdb,
         row_m_z, buffer_percent,
         starts_with("flo"),
         is_lipids)
```

## Is the data balanced? **No**
- We are focusing on predicting if a metabolite is a **lipid Yes/No**. 
```{r}
two_predictor_data %>%
  ggplot(aes(x=is_lipids, fill= is_lipids))+
  geom_bar()

two_predictor_data %>% 
  dplyr::select(is_lipids) %>% 
  tbl_summary(type = list(is_lipids ~ "categorical"))
```
The data outcome **lipid Yes/No** is not balanced, we have an over-representation of the No categories with about 77% of the metabolites which are not lipids. This can lead to a bias in prediction in favor to the No lipid category. We will have to see if balancing the data could improve the model.

<br>
<br>

***
<br>

# 3. Data preprocessing

## What are the steps to take that could improve the predictions?

Investigate if the data needs to be balanced and/or normalized, and/or remove correlated variable. We are going to look if and which preprocessing is useful to improve the models performance:  
1- Normalize numeric data to have a standard deviation of one with *scale* step or ;  
2- Normalize numeric data to have a standard deviation of one and a mean of zero with *normalize* step.  
3- Balanced the outcome variable with *smote* step. This use nearest neighbor to create new synthetic observation almost similar.  
4- Remove correlated variables which can decrease model performance with *corr* step.  

```{r}
set.seed(1234)

# 1. Splitting the data
# 3/4 of the data into the training set but split evenly winthin race
data_split <- initial_split(two_predictor_data, prop = .75, strata = is_lipids)
# Create training and testing datasets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```
So I initialize multiple machine learning recipes including none of those steps, each of those steps alone and crossing steps with each other.
```{r recipe}
recipe_basic <-
  # 1.model formula
  recipe(is_lipids ~ ., data = train_data)  %>%
   # 2.keep these variables but not use them as either outcomes or predictors
  update_role(hmdb, new_role = "ID")

recipe_smote <-
  # 1.model formula
  recipe(is_lipids ~ ., data = train_data)  %>%
   # 2.keep these variables but not use them as either outcomes or predictors
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_smote(is_lipids)

recipe_scale <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors())

recipe_norm <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors())

recipe_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_corr(all_numeric_predictors())

recipe_scale_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_norm_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_corr_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_scale_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors())

recipe_norm_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors())

recipe_norm_smote_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_scale_smote_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)
```

```{r Generate List of recipes}
recipe_list <-
list(basic = recipe_basic,
     balanced = recipe_smote,
     scaled = recipe_scale, normalized = recipe_norm,
     corr.removed = recipe_corr,
     balanced.scaled = recipe_scale_smote, balanced.normalized = recipe_norm_smote,
     balanced.corr = recipe_corr_smote,
     scaled.corr = recipe_scale_corr, normalized.corr = recipe_norm_corr,
     balanced.normalized.corr = recipe_norm_smote_corr,
     balanced.scaled.corr = recipe_scale_smote_corr)
```

Multiple classification models will be evaluated in conjunction with these recipes :  
- bayesian model, decision tree and a trimmed decision tree, support-vector machine, Random forest, boosted tree, elasticnet, logistic regression (lasso, ridge), nearest neighbor.  
Each of these models/recipes will be evaluated on a the training dataset using a 10 fold cross validation approach.
```{r}
nb_spec <-
  naive_Bayes(smoothness = tune(), Laplace = tune()) %>%
  set_engine("klaR") %>%
  set_mode("classification")
```

```{r}
dt_spec <- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

n2_dt_spec <- decision_tree(cost_complexity = tune(), tree_depth = 2, min_n = 2) %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

```{r}
svm_spec <-
  svm_rbf(cost = tune(), rbf_sigma = tune(), margin = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")
```

```{r rand_forest}
ranger_spec <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger")
```

```{r xgboost}
xgboost_spec <-
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(),
             loss_reduction = tune(), sample_size = tune()) %>%
  set_mode("classification") %>%
  set_engine("xgboost")
```

```{r}
glmnet_spec <-
  logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

ridge_spec <-
  logistic_reg(penalty = tune(), mixture = 0) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

lasso_spec <-
  logistic_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")
```

```{r}
knn_spec <-
  nearest_neighbor(neighbors = tune(), weight_func = tune()) %>%
  set_mode("classification") %>%
  set_engine("kknn")
```

```{r Generate list of models}
model_list <-
list(Random_Forest = ranger_spec, SVM = svm_spec, Naive_Bayes = nb_spec,
     Decision_Tree = dt_spec, n2_Decision_Tree = n2_dt_spec,
     Boosted_Trees = xgboost_spec, KNN = knn_spec, Elasticnet = glmnet_spec,
     Ridge_Regression = ridge_spec, Lasso_Regression = lasso_spec)
```

```{r model_set}
model_set <- workflow_set(preproc = recipe_list, models = model_list, cross = T)
```

All combinations of the preprocessors (recipes) and models are compared on their performance to choose the one which perform the best.  


## Machine learning data sets

The known metabolites data is split in to a training dataset (75%) and testing dataset (25%). The split is done evenly within the outcome variable `is_lipids`.
I use a **10 fold cross validation to train and evaluate model on the training set**. Resampling procedure will allow us to fit each workflow to the training data and compute accuracy on each resample. Then we compare the mean of the performance to pick the model/recipe cross combination which perform the best.

```{r cv fold}
set.seed(1234)
mldata_folds <- vfold_cv(train_data, strata = is_lipids)
```

# 4. Train on the training dataset of 1799
```{r run workflows}
doParallel::registerDoParallel()
set.seed(1234)
all_workflows <-
  model_set %>% workflow_map(resamples = mldata_folds,
                             verbose = TRUE)
```

## 4. Visualize performance comparison of combined workflows - Explore Tuning param
```{r, fig.width=12, fig.height=12}
collect_metrics(all_workflows) %>%
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
  filter(.metric == "accuracy") %>%
  group_by(wflow_id) %>%
  filter(mean == max(mean)) %>%
  group_by(model) %>%
  dplyr::select(-.config) %>%
  distinct() %>%
  ungroup() %>%
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  mutate(Model_Type = factor(Model_Type,
                             levels = c("Decision_Tree", "Random_Forest", 
                                        "KNN", "Elasticnet", "Lasso_Regression", 
                                        "n2_Decision_Tree", 
                                        "Boosted_Trees",
                                        "Ridge_Regression", 
                                        "SVM", "Naive_Bayes"
                                        ))) %>%
  filter(!is.na(Model_Type)) %>%
  mutate(Recipe = factor(Recipe,
                             levels = c("basic", "corr.removed", "normalized", "scaled", "balanced",
                                        "normalized.corr", "scaled.corr", "balanced.corr",
                                        "balanced.normalized", "balanced.scaled",
                                        "balanced.normalized.corr", "balanced.scaled.corr"
                                        ))) %>% arrange(desc(mean)) %>% 
  # mutate(mn = 0.912) %>% mutate(mx = 0.94) %>%
  ggplot(aes(x=Workflow_Rank, y = mean, shape = Recipe, color = Model_Type)) +
  # geom_rect(aes(xmin = -Inf, xmax = Inf,
  #           ymin = mn, ymax = mx),
  #           fill= "grey90", color= "transparent")+
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err)) +
  # geom_hline(aes(yintercept = 0.926), color= "darkgrey", linetype= 2)+
  # ylim(0.57, 0.95)+
  theme_classic()+
  scale_colour_manual(values=c("#8DA0CB", "#FC8D62", "#B2DF8A", "#CAB2D6", "#E5C494",
                               "#66C2A5", "#FFD92F", "#E78AC3", "#A6CEE3", "#B3B3B3"))+
  scale_shape_manual(values=c(0, 15, 1, 16, 2, 17, 8, 18, 3, 7, 25, 4))+
  labs(title = "Performance Comparison of Workflow Sets", x = "Workflow Rank",
       y = "Accuracy", color = "Models", shape = "Recipes")

performance_workflows <- collect_metrics(all_workflows) %>%
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
  filter(.metric == "accuracy") %>%
  group_by(wflow_id) %>%
  filter(mean == max(mean)) %>%
  # group_by(model) %>%
  # dplyr::select(-.config) %>%
  # distinct() %>%
  ungroup()

two_predictor_perfmean_DT <- 
  performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% 
  filter(Model_Type == "Decision_Tree") %>% dplyr::select(mean)
two_predictor_perfsd_DT <- 
  performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% 
  filter(Model_Type == "Decision_Tree") %>% dplyr::select(std_err)
two_predictor_perfmean_n2DT <- 
  performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% 
  filter(Model_Type == "n2_Decision_Tree") %>% dplyr::select(mean)
two_predictor_perfsd_n2DT <- 
  performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% 
  filter(Model_Type == "n2_Decision_Tree") %>% dplyr::select(std_err)
```
**Best model and recipe**  
The best models:  
<!-- - Random_Forest with an accurracy of `r as.character(performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "Random_Forest") %>% dplyr::select(mean))`.   -->
- Decision_Tree with an accuracy of `r as.character(round(performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "Decision_Tree") %>% dplyr::select(mean),3))`.  
<!-- <span style="color:green">We decided to move forward with a simpler model which is the Trimmed decision tree. The measured accuracy is `r as.character(round(performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "n2_Decision_Tree") %>% dplyr::select(mean),2))`.</span>      -->
```{r, fig.width=12, fig.height=12}
collect_metrics(all_workflows) %>%
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
  filter(.metric == "accuracy") %>%
  group_by(wflow_id) %>%
  filter(mean == max(mean)) %>%
  group_by(model) %>%
  dplyr::select(-.config) %>%
  distinct() %>%
  ungroup() %>%
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  mutate(Model_Type = factor(Model_Type,
                             levels = c("Decision_Tree", "Random_Forest", 
                                        "KNN", "Elasticnet", "Lasso_Regression", 
                                        "n2_Decision_Tree"
                                        ))) %>%
  filter(!is.na(Model_Type)) %>%
  mutate(Recipe = factor(Recipe,
                             levels = c("basic", "corr.removed", "normalized", "scaled", "balanced",
                                        "normalized.corr", "scaled.corr", "balanced.corr",
                                        "balanced.normalized", "balanced.scaled",
                                        "balanced.normalized.corr", "balanced.scaled.corr"
                                        ))) %>%
  ggplot(aes(x=Recipe, y = mean, shape = Recipe, color = Model_Type)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err),
                width = 0.1) +
  geom_hline(aes(yintercept = as.numeric(two_predictor_perfmean_DT)), color= "#8DA0CB", linetype= 2)+
  theme_classic()+
  theme(axis.text.x = element_blank())+
  scale_colour_manual(values=c("#8DA0CB", "#FC8D62", "#B2DF8A", "#CAB2D6", "#E5C494",
                               "#66C2A5"))+
  scale_shape_manual(values=c(0, 15, 1, 16, 2, 17, 8, 18, 3, 7, 25, 4))+
  labs(title = "Performance Comparison of Workflow Sets", x = NULL,
       y = "Accuracy", color = "Model", shape = "Recipes")+
  guides(color = "none")+
  facet_wrap(. ~ Model_Type, ncol = 3)
```
<span style="color:green">The best **accuracy** can be obtained with the basic recipe. Removing correlated variables, normalizing or scaling the data does not improve the performance. Balancing the data gives worse performance.</span>
<span style="color:green">I choose to continue with the recipe which change the less the data, **basic recipe**.</span>

```{r Extract best tunning param}
set.seed(1234)

final_n2_tree <- all_workflows %>% 
  extract_workflow("basic_n2_Decision_Tree") %>% 
  finalize_workflow(all_workflows %>% 
  extract_workflow_set_result("basic_n2_Decision_Tree") %>% 
  select_best(metric = "accuracy"))

set.seed(1234)

final_tree <- all_workflows %>% 
  extract_workflow("basic_Decision_Tree") %>% 
  finalize_workflow(all_workflows %>% 
  extract_workflow_set_result("basic_Decision_Tree") %>% 
  select_best(metric = "accuracy"))

set.seed(1234)

final_rf <- all_workflows %>%
  extract_workflow("basic_Random_Forest") %>%
  finalize_workflow(all_workflows %>%
  extract_workflow_set_result("basic_Random_Forest") %>%
  select_best(metric = "accuracy"))

set.seed(1234)

final_knn <- all_workflows %>%
  extract_workflow("basic_KNN") %>%
  finalize_workflow(all_workflows %>%
  extract_workflow_set_result("basic_KNN") %>%
  select_best(metric = "accuracy"))
```

These are the hyperpareameters values used:
```{r hyperpareameters}
final_tree
final_rf
# final_n2_tree
# final_knn
```

## 5.Evaluate the best models in depth on training set
### roc_auc, accuracy, sensitivity, specificity, precision
```{r fit_resamples}
set.seed(1234)
n2_tree_results <- final_n2_tree %>%
  fit_resamples(
    resamples = mldata_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity, precision, recall),
    control = control_resamples(save_pred = TRUE, verbose = TRUE)
  )
set.seed(1234)
tree_results <- final_tree %>%
  fit_resamples(
    resamples = mldata_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity, precision, recall),
    control = control_resamples(save_pred = TRUE, verbose = TRUE)
  )
set.seed(1234)
rf_results <- final_rf %>%
  fit_resamples(
    resamples = mldata_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity, precision, recall),
    control = control_resamples(save_pred = TRUE, verbose = TRUE)
  )
set.seed(1234)
knn_results <- final_knn %>%
  fit_resamples(
    resamples = mldata_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
    control = control_resamples(save_pred = TRUE)
  )
```
<br>

```{r Performance Metrics}
n2_tree_results %>% # Compare both models
  collect_metrics() %>%
  dplyr::select(".metric", n2_tree_mean = mean) %>%
  full_join(., rf_results %>%
              collect_metrics() %>%
              dplyr::select(".metric", rf_mean = mean), by = ".metric") %>%
  full_join(., tree_results %>% 
              collect_metrics() %>%
              dplyr::select(".metric", tree_mean = mean), by = ".metric")
  # full_join(., knn_results %>%
  #             collect_metrics() %>%
  #             dplyr::select(".metric", knn_mean = mean), by = ".metric")

n2_tree_results %>% # Compare both models
  collect_metrics() %>%
  mutate(model = "n2_tree") %>%
  bind_rows(rf_results %>%
              collect_metrics() %>%
              mutate(model = "rf")) %>%
  full_join(., tree_results %>% 
              collect_metrics() %>%
              mutate(model = "tree")) %>%
  mutate(model = factor(model, levels= c("tree", "rf",
                                         "n2_tree"))) %>% 
  # bind_rows(knn_results %>%
  #             collect_metrics() %>%
  #             mutate(model = "knn")) %>%

  ggplot(aes(x= .metric, y=mean, fill = model))+
  geom_bar(stat = "identity",
           position = position_dodge())+
  geom_errorbar(aes(x = .metric,
                    ymin = mean - std_err,
                    ymax = mean + std_err),
                width = 0.2, alpha = 0.5,
                position=position_dodge(width=0.9))+
  theme_classic()+
  scale_fill_manual(values = c("#8DA0CB", "#FC8D62", "#66C2A5"), 
                    name= NULL)
  # scale_fill_viridis_d(option = "A", begin= 0.2, end = 0.8)
```
<span style="color:green">roc-auc and accuracy show a really good performance. The sensitivity (ability to detect a true positive) is good but the models suffer from small number of lipid observation in the data which leads to a lower specificity (No lipids are classified as lipids).</span>

```{r evaluate models roc}
n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "n2_tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  full_join(., tree_results %>% 
              collect_predictions() %>%
              mutate(model = "tree")) %>%
  mutate(model = factor(model, levels= c("tree", "rf",
                                         "n2_tree"))) %>% 
  # bind_rows(knn_results %>%
  #             collect_predictions() %>%
  #             mutate(model = "knn")) %>%
  
# rf_results %>% # Compare both models
#   collect_predictions() %>%
#   mutate(model = "rf") %>%
#   bind_rows(knn_results %>%
#               collect_predictions() %>%
#               mutate(model = "glmnet")) %>%
  group_by(model) %>%
  roc_curve(is_lipids, .pred_No) %>%
  autoplot()+
  scale_color_manual(values = c("#8DA0CB", "#FC8D62", "#66C2A5"), name= NULL)+
  ggtitle("Area under Curve ROC (Receiver Operating Characteristics)")
n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "n2_tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  full_join(., tree_results %>% 
              collect_predictions() %>%
              mutate(model = "tree")) %>%
  # bind_rows(knn_results %>%
  #             collect_predictions() %>%
  #             mutate(model = "knn")) %>%
  group_by(model) %>%
  roc_auc(is_lipids, .pred_No)
```
<!-- <span style="color:green">The roc-auc curve for the decision tree model is lower than other models.</span> -->
While ROC shows how the TPR and FPR vary with the threshold, the ROC AUC is a measure of the classification model's ability to distinguish one class from the other. So ROC is not as reliable for unbalanced data.

### PR

PR curves are sensitive to which class is defined as positive, unlike the ROC curve which is a more balanced method. So we will get a completely different result depending on which class is set as positive.
PR curves are sensitive to class distribution, so if the ratio of positives to negatives changes across different analyses, then the PR curve cannot be used to compare performance between them. This is because the chance line varies between datasets with different class distributions.
PR curves, because they use precision, instead of specificity (like ROC) can pick up false positives in the predicted positive fraction. This is very helpful when negatives >> positives. In these cases, the ROC is pretty insensitive and can be misleading, whereas PR curves reign supreme.



Precision-Recall is a useful measure of success of prediction when the classes are **very imbalanced**.
precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.

The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.

```{r evaluate models pr}
n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "n2_tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  full_join(., tree_results %>% 
              collect_predictions() %>%
              mutate(model = "tree")) %>%
  mutate(model = factor(model, levels= c("tree", "rf", "n2_tree"))) %>% 
  # bind_rows(knn_results %>%
  #             collect_predictions() %>%
  #             mutate(model = "knn")) %>%
  group_by(model) %>%
  pr_curve(is_lipids, .pred_Yes) %>%
  autoplot()+
  scale_color_manual(values = c("#8DA0CB", "#FC8D62", "#66C2A5"), name= NULL)+
  ggtitle("Area under the precision recall curve - \nYes prediction")
n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "n2_tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  full_join(., tree_results %>% 
              collect_predictions() %>%
              mutate(model = "tree")) %>%
  # bind_rows(knn_results %>%
  #             collect_predictions() %>%
  #             mutate(model = "knn")) %>%
  group_by(model) %>%
  average_precision(is_lipids, .pred_Yes)

n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "n2_tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  full_join(., tree_results %>% 
              collect_predictions() %>%
              mutate(model = "tree")) %>%
  mutate(model = factor(model, levels= c("tree", "rf", "n2_tree"))) %>% 
  # bind_rows(knn_results %>%
  #             collect_predictions() %>%
  #             mutate(model = "knn")) %>%
  group_by(model) %>%
  pr_curve(is_lipids, .pred_No) %>%
  autoplot()+
  scale_color_manual(values = c("#8DA0CB", "#FC8D62", "#66C2A5"), name= NULL)+
  ggtitle("Area under the precision recall curve - \nNo prediction")
n2_tree_results %>% # Compare both models
  collect_predictions() %>%
  mutate(model = "n2_tree") %>%
  bind_rows(rf_results %>%
              collect_predictions() %>%
              mutate(model = "rf")) %>%
  full_join(., tree_results %>% 
              collect_predictions() %>%
              mutate(model = "tree")) %>%
  # bind_rows(knn_results %>%
  #             collect_predictions() %>%
  #             mutate(model = "knn")) %>%
  group_by(model) %>%
  average_precision(is_lipids, .pred_No)
```
<!-- <span style="color:green">For what I can see here, the prediction for No is very precise when the prediction for Yes is less precise.</span>   -->
<span style="color:green">The Trimmed decision tree exhibit a good PR.</span> 
<span style="color:red">I am then moving forward with the Trimmed decision tree.</span>
<br>
<br>


# 4. Fit on the whole training / predict on testing data (1799 data)
## Analyse results
```{r}
set.seed(1234)

# final_fit <- all_workflows %>% 
#   extract_workflow("basic_Random_Forest") %>% 
#   finalize_workflow(all_workflows %>% 
#                       extract_workflow_set_result("basic_Random_Forest") %>% 
#                       select_best(metric = "accuracy")) %>% 
#   fit(train_data)


final_fit <- all_workflows %>% 
  extract_workflow("basic_n2_Decision_Tree") %>% 
  finalize_workflow(all_workflows %>% 
                      extract_workflow_set_result("basic_n2_Decision_Tree") %>% 
                      select_best(metric = "accuracy")) %>% 
  last_fit(data_split)
```
## 1.Look at performance with collect_metrics on the testing data (can see if over fit)
```{r}
collect_metrics(final_fit) %>% 
  bind_rows(collect_predictions(final_fit) %>% 
              specificity(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(collect_predictions(final_fit) %>% 
              sensitivity(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(collect_predictions(final_fit) %>% 
              average_precision(is_lipids, .pred_No)) %>% 
  bind_rows(collect_predictions(final_fit) %>% 
              precision(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(collect_predictions(final_fit) %>% 
              recall(truth = is_lipids, estimate = .pred_class)) %>% 
  `colnames<-`(c("metric", "estimator", "performance", "config")) %>% 
  dplyr::select(-c(estimator, config))
```
As a reminder, performance on the training set was:
```{r}
# Compare to the training previous number
collect_metrics(n2_tree_results) %>% 
  `colnames<-`(c("metric", "estimator", "performance", "n", "SD", "config")) %>% 
  dplyr::select(-c(estimator, n, config))
```
The model is not overfitting (performance with the testing set is not lower than with the training set - or within the SD aka specificity).
```{r}
collect_metrics(final_fit) %>% 
  bind_rows(collect_predictions(final_fit) %>% 
              specificity(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(collect_predictions(final_fit) %>% 
              sensitivity(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(collect_predictions(final_fit) %>% 
              precision(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(collect_predictions(final_fit) %>% 
              recall(truth = is_lipids, estimate = .pred_class)) %>% 

  `colnames<-`(c("metric", "estimator", "performance", "config")) %>% 
  mutate(model = "testing") %>% 
  bind_rows(.,
            collect_metrics(n2_tree_results) %>% 
              `colnames<-`(c("metric", "estimator", "performance", "n", "std_err", "config")) %>% 
              mutate(model = "training")
              ) %>% 
  mutate(model = factor(model, levels = c("training", "testing"))) %>% 
  # mutate(metric = str_replace(metric, "average_precision", "precision")) %>% 

  ggplot(aes(x= metric, y=performance, fill = model))+
  geom_bar(stat = "identity",
           position = position_dodge())+
  geom_errorbar(aes(x = metric,
                    ymin = performance - std_err,
                    ymax = performance + std_err),
                width = 0.2, alpha = 0.5,
                position=position_dodge(width=0.9))+
  scale_fill_viridis_d(option = "G", begin = 0.3, end = 0.8, 
                        name = NULL)
```

## 2.Get predictions out on the testing dataset
```{r}
df <- final_fit %>%
  collect_predictions()
conf_tab <- table(df$is_lipids, df$.pred_class)
conf_tab <- conf_tab / rowSums(conf_tab)
conf_tab <- as.data.frame(conf_tab, stringsAsFactors = TRUE)
conf_tab$Var2 <- factor(conf_tab$Var2, rev(levels(conf_tab$Var2)))

ggplot(conf_tab, aes(Var1, Var2, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = scales::percent(Freq))) +
  scale_fill_gradient(low = "white", high = "#3575b5")+
  labs(x = "Truth", y = "Prediction", title = "Confusion matrix using Trimmed Decision Tree",
       fill = "Valid prediction")+
  theme_classic()

final_fit %>%
  collect_predictions() %>%
  conf_mat(is_lipids, .pred_class)
```
<!-- ## 3.Look at variable importance -->
<!-- ```{r} -->
<!-- dt_fit <-  -->
<!--   final_fit %>%  -->
<!--   extract_fit_parsnip() -->
<!-- #Generate Decision Tree Plot Using rpart.plot package -->
<!-- rpart.plot::rpart.plot(dt_fit$fit) -->

<!-- ``` -->

## 4.What is the m/z-buffer_percent pattern of predicted data over training data?
```{r}
training(data_split) %>% 
  mutate(data = "training") %>% 
  bind_rows(testing(data_split) %>% 
              mutate(data = "testing")) %>% 
  ggplot(aes(x=row_m_z, buffer_percent, color= data))+
  geom_point()+
  # scale_color_discrete(name = NULL)+
  scale_color_viridis_d(option = "G", begin = 0.3, end = 0.8, 
                        name = NULL)+
  facet_wrap(. ~ is_lipids, ncol = 2)
```
What is the m/z-rentention time pattern of predicted data over real data?
```{r}
final_fit %>%
  collect_predictions() %>% dplyr::select(-is_lipids) %>% 
  bind_cols(testing(data_split)) %>% 
  mutate(concordant = case_when(
    is_lipids == .pred_class      ~ "truly classified",
    is_lipids != .pred_class      ~ "wrongly classified",
  )) %>% 
  ggplot(aes(x=row_m_z, buffer_percent, color= concordant))+
  geom_point()+
  scale_color_manual(name = NULL, values= c("#3399FF", "tomato"))+
  # scale_color_viridis_d(option = "A", 
  #                       begin = 0.2, end = 0.7,
  #                       name = NULL)+
  facet_wrap(. ~ is_lipids, ncol = 2)

final_fit %>%
  collect_predictions() %>% dplyr::select(-is_lipids) %>% 
  bind_cols(testing(data_split)) %>% 
  mutate(concordant = case_when(
    is_lipids == .pred_class      ~ "truly classified",
    is_lipids != .pred_class      ~ "wrongly classified",
  )) %>% 
  dplyr::select(hmdb, row_m_z, buffer_percent, is_lipids, .pred_class, concordant) %>% 
  arrange(desc(concordant))
```

# 5. Prediction on validation data 3990 with Trimmed Decision Tree
The new data is cleaned in the same manner as the data used to create the machine learning model.  
- keep row_retention_time > 1 & < 14  
- keep only metabolite row entry when positive and negative metabolites are present

```{r}
tbl <- validation3990_data %>% 
  dplyr::select(non_heavy_identified_flag) %>% 
  mutate(non_heavy_identified_flag = case_when(
    non_heavy_identified_flag == 1            ~ "identified",
    TRUE                                      ~ "non identified"
  )) %>% 
  tbl_summary()
tbl

validation3990_data %>% 
  filter(non_heavy_identified_flag == 1) %>% 
  dplyr::select(average_molecular_weight, monisotopic_molecular_weight, 
                taxonomy_class, taxonomy_direct_parent, 
                taxonomy_molecular_framework, taxonomy_sub_class,
                taxonomy_super_class) %>% 
  tbl_summary(sort = list(everything() ~ "frequency")) %>% 
  modify_header(
    label = "**Known Metabolites Characteristics in 3990 data**"
  )
```


```{r predict new data}
final_fitted <- extract_workflow(final_fit)

set.seed(1234)
predicted_validation3990_data <- augment(final_fitted, validation3990_data)
```

Performance
```{r performance validation}
predicted_validation3990_data %>% 
  accuracy(truth = is_lipids, estimate = .pred_class) %>% 
  bind_rows(predicted_validation3990_data %>% 
              roc_auc(is_lipids, .pred_No)) %>% 
  bind_rows(predicted_validation3990_data %>% 
              specificity(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(predicted_validation3990_data %>% 
              sensitivity(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(predicted_validation3990_data %>% 
              average_precision(is_lipids, .pred_No)) %>% 
  bind_rows(predicted_validation3990_data %>% 
              precision(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(predicted_validation3990_data %>% 
              recall(truth = is_lipids, estimate = .pred_class))

predicted_validation3990_data %>% 
  accuracy(truth = is_lipids, estimate = .pred_class) %>% 
  bind_rows(predicted_validation3990_data %>% 
              roc_auc(is_lipids, .pred_No)) %>% 
  bind_rows(predicted_validation3990_data %>% 
              specificity(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(predicted_validation3990_data %>% 
              sensitivity(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(predicted_validation3990_data %>% 
              average_precision(is_lipids, .pred_No)) %>% 
  bind_rows(predicted_validation3990_data %>% 
              precision(truth = is_lipids, estimate = .pred_class)) %>% 
  bind_rows(predicted_validation3990_data %>% 
              recall(truth = is_lipids, estimate = .pred_class)) %>% 
  ggplot(aes(x=.metric, y=.estimate, fill=.metric))+
  geom_bar(stat = "identity")+
  ggtitle("Performance on validation data")+
  labs(x= "", y= "estimate", fill = NULL)+
  scale_fill_viridis_d(option = "F")+
  coord_flip()

# predicted_validation3990_data %>% 
#   roc_curve(is_lipids, .pred_No) %>%
#   autoplot()


# predicted_validation3990_data %>% 
#   pr_curve(is_lipids, .pred_No) %>%
#   autoplot()
```




get predictions out collect_predictions
```{r conf mat validation}
# df <- final_fit %>%
#   collect_predictions()
conf_tab <- table(predicted_validation3990_data$is_lipids, predicted_validation3990_data$.pred_class)
conf_tab <- conf_tab / rowSums(conf_tab)
conf_tab <- as.data.frame(conf_tab, stringsAsFactors = TRUE)
conf_tab$Var2 <- factor(conf_tab$Var2, rev(levels(conf_tab$Var2)))

ggplot(conf_tab, aes(Var1, Var2, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = scales::percent(Freq))) +
  scale_fill_gradient(low = "white", high = "#3575b5")+
  labs(x = "Truth", y = "Prediction", title = "Confusion matrix using Trimmed Decision Tree",
       fill = "Valid prediction")+
  theme_classic()

predicted_validation3990_data %>%
  conf_mat(is_lipids, .pred_class)
```
<!-- look at var importance -->
<!-- ```{r draw tree validation} -->
<!-- dt_fit <-  -->
<!--   final_fitted %>%  -->
<!--   extract_fit_parsnip() -->
<!-- #Generate Decision Tree Plot Using rpart.plot package -->
<!-- rpart.plot::rpart.plot(dt_fit$fit, roundint = FALSE) -->
<!-- # probability per class of observations in the node -->
<!-- rpart.plot::rpart.plot(dt_fit$fit, roundint = FALSE, extra = 1) -->
<!-- rpart.plot::rpart.plot(dt_fit$fit, roundint = FALSE, extra = 0) -->
<!-- # rpart.plot::rpart.plot(dt_fit$fit, roundint = FALSE, box.palette = c("grey", "slateblue", "tomato")) -->
<!-- # rpart.plot::rpart.plot(dt_fit$fit, roundint = FALSE, box.palette = "YlGnBl") -->
<!-- # rpart.plot::rpart.plot(dt_fit$fit, roundint = FALSE, box.palette = "RdYlGn") -->
<!-- # rpart.plot::rpart.plot(dt_fit$fit, roundint = FALSE, prefix="Lipids ", border.col= "blue") -->
<!-- ``` -->

What is the m/z-rentention time pattern of predicted data?
```{r plot validation}
predicted_validation3990_data %>% 
  mutate(concordant = case_when(
    is_lipids == .pred_class      ~ "truly classified",
    is_lipids != .pred_class      ~ "wrongly classified",
  )) %>% 
  ggplot(aes(x=row_m_z, row_retention_time, color= concordant))+
  geom_point()+
  scale_color_manual(name = NULL, values= c("#3399FF", "tomato"))+
  # scale_color_discrete(name = NULL)+
  facet_wrap(. ~ is_lipids, ncol = 2)

predicted_validation3990_data %>% 
  mutate(concordant = case_when(
    is_lipids == .pred_class      ~ "truly classified",
    is_lipids != .pred_class      ~ "wrongly classified",
  )) %>% 
  dplyr::select(hmdb, row_identity_all_i_ds, row_m_z, buffer_percent, is_lipids, .pred_class, concordant) %>% 
  arrange(desc(concordant), is_lipids)
```


# Part II
**Here we are looking at if we increase the prediction of unknown metabolites using the Samples data (expression) on top of retention time and m/z information as predictors.**

```{r split with samples}
set.seed(1234)

# 1. Splitting the data
# 3/4 of the data into the training set but split evenly winthin race
data_split <- initial_split(sample_predictor_data, prop = .75, strata = is_lipids)
# Create training and testing datasets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

```{r recipe part2}
recipe_basic <-
  # 1.model formula
  recipe(is_lipids ~ ., data = train_data)  %>%
   # 2.keep these variables but not use them as either outcomes or predictors
  update_role(hmdb, new_role = "ID")

recipe_smote <-
  # 1.model formula
  recipe(is_lipids ~ ., data = train_data)  %>%
   # 2.keep these variables but not use them as either outcomes or predictors
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_smote(is_lipids)

recipe_scale <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors())

recipe_norm <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors())

recipe_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_corr(all_numeric_predictors())

recipe_scale_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_norm_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_corr_smote <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_scale_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors())

recipe_norm_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors())

recipe_norm_smote_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)

recipe_scale_smote_corr <-
  recipe(is_lipids ~ ., data = train_data)  %>%
  update_role(hmdb, new_role = "ID") %>%
  # 3. Set differential steps
  step_scale(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_smote(is_lipids)
```

```{r Generate List of recipes part2}
recipe_list <-
list(basic = recipe_basic,
     balanced = recipe_smote,
     scaled = recipe_scale, normalized = recipe_norm,
     corr.removed = recipe_corr,
     balanced.scaled = recipe_scale_smote, balanced.normalized = recipe_norm_smote,
     balanced.corr = recipe_corr_smote,
     scaled.corr = recipe_scale_corr, normalized.corr = recipe_norm_corr,
     balanced.normalized.corr = recipe_norm_smote_corr,
     balanced.scaled.corr = recipe_scale_smote_corr
     )
```


```{r dtree with samples}
n2_dt_spec <- decision_tree(cost_complexity = tune(), tree_depth = 2, min_n = 2) %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

```{r Generate list of models part2}
model_list <-
  list(n2_Decision_Tree = n2_dt_spec)
```

```{r model_set part2}
model_set <- workflow_set(preproc = recipe_list, models = model_list, cross = T)
```

```{r cv fold part2}
set.seed(1234)
mldata_folds <- vfold_cv(train_data, strata = is_lipids)
```


```{r run workflows part2}
doParallel::registerDoParallel()
set.seed(1234)
all_workflows <-
  model_set %>% workflow_map(resamples = mldata_folds,
                             verbose = TRUE)
```

### Using the same model on the training data including the expression data this time we observe no increased performance.
### As previously, the basic recipe gives the best performance

```{r performance part2, fig.width=12, fig.height=6}
collect_metrics(all_workflows) %>%
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
  filter(.metric == "accuracy") %>%
  group_by(wflow_id) %>%
  filter(mean == max(mean)) %>%
  group_by(model) %>%
  dplyr::select(-.config) %>%
  distinct() %>%
  ungroup() %>%
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  filter(!is.na(Model_Type)) %>%
  mutate(Recipe = factor(Recipe,
                             levels = c("basic", "corr.removed", "normalized", "scaled", "balanced",
                                        "normalized.corr", "scaled.corr", "balanced.corr",
                                        "balanced.normalized", "balanced.scaled",
                                        "balanced.normalized.corr", "balanced.scaled.corr"
                                        ))) %>% arrange(desc(mean)) %>% 
  # mutate(mn = 0.912) %>% mutate(mx = 0.94) %>%
  ggplot(aes(x=Workflow_Rank, y = mean, shape = Recipe)) +
  geom_rect(aes(xmin = -Inf, xmax = Inf,
                ymin = as.numeric(two_predictor_perfmean_n2DT) - as.numeric(two_predictor_perfsd_n2DT),
                ymax = as.numeric(two_predictor_perfmean_n2DT) + as.numeric(two_predictor_perfsd_n2DT)),
            fill= "grey90", color= "transparent")+
  geom_point(size = 3, color= "#8DA0CB") +
  geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err), color= "#8DA0CB") +
  geom_hline(aes(yintercept = as.numeric(two_predictor_perfmean_n2DT)), color= "#E5C494", linetype= 2)+
  # ylim(0.57, 0.95)+
  theme_classic()+
  scale_shape_manual(values=c(0, 15, 1, 16, 2, 17, 8, 18, 3, 7, 25, 4))+
  labs(title = "Performance Comparison of Workflow Sets", x = "Workflow Rank",
       y = "Accuracy", color = "Models", shape = "Recipes")

performance_workflows <- collect_metrics(all_workflows) %>%
  separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
  filter(.metric == "accuracy") %>%
  group_by(wflow_id) %>%
  filter(mean == max(mean)) %>%
  group_by(model) %>%
  dplyr::select(-.config) %>%
  distinct() %>%
  ungroup() %>%
  mutate(Workflow_Rank =  row_number(-mean),
         .metric = str_to_upper(.metric)) %>%
  filter(!is.na(Model_Type)) %>%
  mutate(Recipe = factor(Recipe,
                             levels = c("basic", "corr.removed", "normalized", "scaled", "balanced",
                                        "normalized.corr", "scaled.corr", "balanced.corr",
                                        "balanced.normalized", "balanced.scaled",
                                        "balanced.normalized.corr", "balanced.scaled.corr"
                                        ))) %>% arrange(desc(mean))
```
**Adding the samples as predictors with the Trimmed decision tree model** has an accuracy of `r as.character(round(performance_workflows %>% distinct(Model_Type, .keep_all = TRUE) %>% filter(Model_Type == "n2_Decision_Tree") %>% dplyr::select(mean), 2))`.
As a reminder, the same model trained on the dataset containing m/z and buffer percent only had an average accuracy of `r as.character(round(two_predictor_perfmean_n2DT,2))`+/- `r as.character(round(two_predictor_perfsd_n2DT,2))` showed in the grey dotted line and rectangle.  
Adding the samples as predictors gives a similar performance of the Trimmed decision tree model.



